@misc{wittig2023progress,
      title={Progress on $(g-2)_\mu$ from Lattice QCD}, 
      author={Hartmut Wittig},
      year={2023},
      eprint={2306.04165},
      archivePrefix={arXiv},
      primaryClass={hep-ph}
}
@article{Duane:1987de,
    author = "Duane, S. and Kennedy, A. D. and Pendleton, B. J. and Roweth, D.",
    title = "{Hybrid Monte Carlo}",
    doi = "10.1016/0370-2693(87)91197-X",
    journal = "Phys. Lett. B",
    volume = "195",
    pages = "216--222",
    year = "1987"
}

@article{Shanahan:2022ifi,
    author = "Shanahan, Phiala and others",
    title = "{Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning}",
    eprint = "2209.07559",
    archivePrefix = "arXiv",
    primaryClass = "physics.comp-ph",
    reportNumber = "FERMILAB-CONF-22-719-ND-PPD-QIS-SCD",
    month = "9",
    year = "2022"
}

@inproceedings{Boyda:2022nmh,
    author = "Boyda, Denis and others",
    title = "{Applications of Machine Learning to Lattice Quantum Field Theory}",
    booktitle = "{Snowmass 2021}",
    eprint = "2202.05838",
    archivePrefix = "arXiv",
    primaryClass = "hep-lat",
    reportNumber = "MIT-CTP/5405",
    month = "2",
    year = "2022"
}

@article{Foreman:2021rhs,
    author = "Foreman, Sam and Jin, Xiao-Yong and Osborn, James C.",
    title = "{LeapfrogLayers: A Trainable Framework for Effective Topological Sampling}",
    eprint = "2112.01582",
    archivePrefix = "arXiv",
    primaryClass = "hep-lat",
    doi = "10.22323/1.396.0508",
    journal = "PoS",
    volume = "LATTICE2021",
    pages = "508",
    month = "05",
    year = "2022",
}


@article{Foreman:2021ljl,
    author = "Foreman, Sam and Izubuchi, Taku and Jin, Luchang and Jin, Xiao-Yong and Osborn, James C. and Tomiya, Akio",
    title = "{HMC with Normalizing Flows}",
    eprint = "2112.01586",
    archivePrefix = "arXiv",
    primaryClass = "cs.LG",
    doi = "10.22323/1.396.0073",
    journal = "PoS",
    volume = "LATTICE2021",
    pages = "073",
    year = "2022"
}

@inproceedings{Foreman:2021ixr,
    author = "Foreman, Sam and Jin, Xiao-Yong and Osborn, James C.",
    title = "{Deep Learning Hamiltonian Monte Carlo}",
    booktitle = "{9th International Conference on Learning Representations}",
    eprint = "2105.03418",
    archivePrefix = "arXiv",
    primaryClass = "hep-lat",
    month = "5",
    year = "2021"
}

https://towardsdatascience.com/mastering-language-models-32e1d891511a
@misc{Montgomery_2023,
    title={Mastering language models},
    url={https://towardsdatascience.com/mastering-language-models-32e1d891511a},
    journal={Medium},
    publisher={Towards Data Science},
    author={Montgomery, Samuel}, year={2023},
    month={Oct}
}

@misc{yang2023harnessing,
      title={Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond}, 
      author={Jingfeng Yang and Hongye Jin and Ruixiang Tang and Xiaotian Han and Qizhang Feng and Haoming Jiang and Bing Yin and Xia Hu},
      year={2023},
      eprint={2304.13712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Popel_2018,
    doi = {10.2478/pralin-2018-0002},
    url = {https://doi.org/10.2478%2Fpralin-2018-0002}
    ,
    year = 2018,
    month = {apr},
    publisher = {Charles University in Prague, Karolinum Press},
    volume = {110},
    number = {1},
    pages = {43--70},
    author = {Martin Popel and Ond{\v{r}}ej Bojar},
    title = {Training Tips for the Transformer Model},
    journal = {The Prague Bulletin of Mathematical Linguistics}
}
@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{yao2023tree,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article {Zvyagin2022.10.10.511571,
    author = {Maxim Zvyagin and Alexander Brace and Kyle Hippe and Yuntian Deng and Bin Zhang and Cindy Orozco Bohorquez and Austin Clyde and Bharat Kale and Danilo Perez-Rivera and Heng Ma and Carla M. Mann and Michael Irvin and J. Gregory Pauloski and Logan Ward and Valerie Hayot-Sasson and Murali Emani and Sam Foreman and Zhen Xie and Diangen Lin and Maulik Shukla and Weili Nie and Josh Romero and Christian Dallago and Arash Vahdat and Chaowei Xiao and Thomas Gibbs and Ian Foster and James J. Davis and Michael E. Papka and Thomas Brettin and Rick Stevens and Anima Anandkumar and Venkatram Vishwanath and Arvind Ramanathan},
    title = {GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics},
    elocation-id = {2022.10.10.511571},
    year = {2022},
    doi = {10.1101/2022.10.10.511571},
    publisher = {Cold Spring Harbor Laboratory},
    abstract = {We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.},
    URL = {https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571},
    eprint = {https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571.full.pdf},
    journal = {bioRxiv}
}
