<!DOCTYPE html>
<html lang="en"><head>
<link href="././favicon.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/quarto-contrib/roughnotation-0.5.1/rough-notation.iife.js"></script>
<script src="site_libs/quarto-contrib/roughnotation-init-1.0.0/rough.js"></script>
<script src="site_libs/quarto-contrib/iconify-1.0.8/iconify-icon.min.js"></script>
<link href="site_libs/quarto-contrib/academicons-1.9.2/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/academicons-1.9.2/size.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/bootstrap-icons-1.11.1/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.549">

  <meta name="author" content="Sam Foreman ">
  <meta name="dcterms.date" content="2024-03-12">
  <title>Parallel Training Techniques</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #383a42;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #383a42; } /* Normal */
    code span.al { color: #95da4c; background-color: #4d1f24; font-weight: bold; } /* Alert */
    code span.an { color: #50a14f; } /* Annotation */
    code span.at { color: #a626a4; } /* Attribute */
    code span.bn { color: #986801; } /* BaseN */
    code span.bu { color: #a626a4; } /* BuiltIn */
    code span.cf { color: #a626a4; } /* ControlFlow */
    code span.ch { color: #50a14f; } /* Char */
    code span.cn { color: #986801; } /* Constant */
    code span.co { color: #a0a1a7; font-style: italic; } /* Comment */
    code span.cv { color: #e45649; font-style: italic; } /* CommentVar */
    code span.do { color: #e45649; } /* Documentation */
    code span.dt { color: #a626a4; } /* DataType */
    code span.dv { color: #986801; } /* DecVal */
    code span.er { color: #f44747; text-decoration: underline; } /* Error */
    code span.ex { color: #4078f2; font-weight: bold; } /* Extension */
    code span.fl { color: #986801; } /* Float */
    code span.fu { color: #4078f2; } /* Function */
    code span.im { color: #50a14f; } /* Import */
    code span.in { color: #c45b00; } /* Information */
    code span.kw { color: #a626a4; } /* Keyword */
    code span.op { color: #a626a4; } /* Operator */
    code span.ot { color: #27ae60; } /* Other */
    code span.pp { color: #a626a4; } /* Preprocessor */
    code span.re { color: #2980b9; background-color: #153042; } /* RegionMarker */
    code span.sc { color: #0184bc; } /* SpecialChar */
    code span.ss { color: #da4453; } /* SpecialString */
    code span.st { color: #50a14f; } /* String */
    code span.va { color: #e45649; } /* Variable */
    code span.vs { color: #da4453; } /* VerbatimString */
    code span.wa { color: #da4453; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="css/default.css">
  <link rel="stylesheet" href="css/reset.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XVM2Y822Y1"></script>

  <script type="text/javascript">

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-XVM2Y822Y1', { 'anonymize_ip': true});
  </script>
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <meta name="mermaid-theme" content="neutral">
  <script src="site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<meta property="og:title" content="Parallel Training Techniques">
<meta property="og:site_name" content="Parallel Training Techniques">
<meta name="twitter:title" content="Parallel Training Techniques">
<meta name="twitter:image" content="https://saforem2.github.io/parallel-training-slides/assets/thumbnail.png">
<meta name="twitter:creator" content="@saforem2">
<meta name="twitter:site" content="@saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Parallel Training Techniques">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2024-03-12">
<meta name="citation_cover_date" content="2024-03-12">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-03-12">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/parallel-training-slides">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Progress on (g-2)_\mu from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022-05;,citation_cover_date=2022-05;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ond≈ôej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Parallel Training Techniques</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<a href="https://samforeman.me">Sam Foreman </a><a href="https://orcid.org/0000-0002-9981-0876"><span class="orcid-green"><i class="ai  ai-orcid"></i></span></a> 
</div>
<div class="quarto-title-author-email">
<a href="mailto:foremans@anl.gov">foremans@anl.gov</a>
</div>
        <p class="quarto-title-affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            Argonne National Laboratory
            </a>
          </p>
    </div>
</div>

  <p class="date">2024-03-12</p>
</section>
<section id="parallel-training-techniques" class="title-slide slide level1 center">
<h1>Parallel Training Techniques</h1>
<div class="quarto-layout-panel" data-layout="[60,-5, 35]" style="display: flex; align-items:flex-end;">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="col1 quarto-layout-cell" style="flex-basis: 60.0%;justify-content: flex-start;">
<p><span class="dim-text"><a href="https://github.com/argonne-lcf/llm-workshop"><strong>AI for Science Training Series</strong></a></span><br>
<span class="dim-text">Argonne National Laboratory</span><br>
<br></p>
<p><span style="font-weight: 600;"><a href="https://samforeman.me"><i class="bi-person-badge " style="" role="img" aria-hidden="true"></i>Sam Foreman</a></span><br>
<span class="dim-text" style="font-size: 0.8em;">[2024-03-12]</span></p>
<p><br></p>
<div style="font-size: 0.75em;">
<ul>
<li><a href="https://github.com/argonne-lcf/"><iconify-icon inline="" icon="line-md:home-md-twotone-alt"></iconify-icon><code>argonne-lcf/</code></a>
<ul>
<li><a href="https://github.com/argonne-lcf/ai-science-training-series"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>ai-science-training-series/</code></a>
<ul>
<li><a href="https://github.com/argonne-lcf/ai-science-training-series/blob/main/06_parallel_training_methods/README.md"><iconify-icon inline="" icon="line-md:document"></iconify-icon><code>06_parallel_training_methods/</code></a></li>
</ul></li>
</ul></li>
<li><a href="https://github.com/saforem2"><iconify-icon inline="" icon="line-md:person"></iconify-icon><code>saforem2/</code></a>
<ul>
<li><a href="https://github.com/saforem2/parallel-training-slides"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>parallel-training-slides</code></a>
<ul>
<li><a href="https://saforem2.github.io/parallel-training-slides/"><iconify-icon inline="" icon="line-md:image-twotone"></iconify-icon>[<strong>slides</strong>]</a> <a href="https://github.com/saforem2/parallel-training-slides/"><iconify-icon inline="" icon="line-md:github-twotone"></iconify-icon>[<strong>GitHub</strong>]</a> <!-- - [\[ slides\]](https://github.com/saforem2/llm-workshop-talk) --> <!-- - [ `llm-workshop-talk`] --> <!-- - [\[ slides\]](https://github.com/saforem2/llm-workshop-talk)  --></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="col2 quarto-layout-cell" style="flex-basis: 35.0%;justify-content: flex-start;">
<div id="fig-3d-parallel" class="quarto-figure quarto-figure-center quarto-float" style="text-align: center;">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-3d-parallel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="https://www.microsoft.com/en-us/research/uploads/prod/2020/09/Blog_DeepSpeed3_Figure2_highres.png">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3d-parallel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <span class="dim-text"><a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">Source</a></span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>

<section id="ai-compute" class="title-slide slide level1 centeredslide center" height="100%">
<h1>AI ü§ù Compute</h1>
<p><img data-src="./assets/ai-and-compute-all.png" style="width:70.0%"></p>
<div class="dim-text" style="font-size: 0.55em;">
<p>[‚Ä¶] since 2012, the amount of [AI] compute used has been increasing exponentially with a 3.4-month doubling time<sup>1</sup>, or [<strong>300,000</strong>x]. <a href="https://openai.com/research/ai-and-compute">Source.</a></p>
</div>
<aside><ol class="aside-footnotes"><li id="fn1"><p>By comparison, Moore‚Äôs Law had a 2-year doubling period, and would have doubled 7x since 2012</p></li></ol></aside></section>

<section id="ai-compute-modern-era" class="title-slide slide level1 centeredslide center" height="100%">
<h1>AI ü§ù Compute [Modern Era]</h1>

<img data-src="./assets/ai-and-compute-modern-log.png" style="width:90.0%" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;2: [<strong>300,000</strong>x since 2012] vs [7x for Moore‚Äôs Law]. <a href="https://openai.com/research/ai-and-compute">Source.</a>
</p></section>

<section id="single-gpu-training" class="title-slide slide level1 centeredslide center">
<h1>Single GPU Training</h1>

<img data-src="./assets/single-gpu-step-1.drawio.svg" style="width:90.0%" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;3: <strong>SLOW</strong> !! model size limited by GPU memory
</p></section>

<section id="collective-communication" class="title-slide slide level1 smaller center">
<h1>Collective Communication</h1>
<p>Typically, we assign 1 <code>rank</code> to each GPU (or <code>accelerator</code>), i.e.&nbsp;<code>rank</code> <span class="math inline">\in</span> <code>[0, 1, ..., WORLD_SIZE-1]</code>.</p>
<div class="panel-tabset">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1"><code>AllReduce</code></a></li><li><a href="#tabset-1-2"><code>Reduce</code></a></li><li><a href="#tabset-1-3"><code>Broadcast</code></a></li><li><a href="#tabset-1-4"><code>AllGather</code></a></li><li><a href="#tabset-1-5"><code>Scatter</code></a></li></ul>
<div class="tab-content">
<div id="tabset-1-1">
<ul>
<li>Perform <em>reductions</em> on data (e.g.&nbsp;<code>sum</code>, <code>min</code>, <code>max</code>) across ranks, send result back to everyone</li>
</ul>
<div id="fig-allreduce" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-allreduce-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="./assets/collective-allreduce-sum.drawio.svg" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-allreduce-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: All-Reduce operation: each rank receives the reduction of input values across ranks.
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-2">
<ul>
<li>Perform a <em>reduction</em> on data across ranks, send to individual</li>
</ul>
<div id="fig-reduce" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-reduce-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="./assets/collective-reduce-sum.drawio.svg" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-reduce-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Reduce operation: one rank receives the reduction of input values across ranks
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-3">
<ul>
<li><code>broadcast</code> (<em>send</em>) a tensor <code><span class="math inline">x</span></code> from one rank to all ranks</li>
</ul>
<div id="fig-broadcast" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-broadcast-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="./assets/collective-broadcast.drawio.svg" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-broadcast-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-4">
<ul>
<li>Gathers tensors from the whole group in a list.</li>
</ul>
<div id="fig-allgather" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-allgather-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="./assets/collective-allgather.drawio.svg" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-allgather-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-5">
<ul>
<li>Scatters a list of tensors to the whole group</li>
</ul>
<div id="fig-scatter" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="./assets/collective-scatter.drawio.svg" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>

<section id="collective-operations" class="title-slide slide level1 center">
<h1>Collective Operations</h1>
<div title="‚åõ Timeouts">
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>‚åõ Timeouts</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Collective operations have to be called for each <code>rank</code> to form a complete collective operation.
<ul>
<li>Failure to do so will result in other ranks waiting <strong>indefinitely</strong></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</section>

<section id="why-distributed-training" class="title-slide slide level1 center">
<h1>Why Distributed Training?</h1>
<ul>
<li>Splitting data across workers <span class="math inline">\longrightarrow</span> larger batch size<sup>1</sup>
<ul>
<li>[<code>micro_batch_size = 1</code>] <span class="math inline">\times</span> [<code>N</code> GPUs] <span class="math inline">\rightarrow</span> [<b><code>global_batch_size = N</code></b>]</li>
</ul></li>
<li>Smooth loss landscape</li>
<li>Improved gradient estimators</li>
<li>Less iterations needed for same number of epochs
<ul>
<li>May need to train for more epochs if another change is not made</li>
<li>e.g.&nbsp;scaling learning rate</li>
</ul></li>
<li>See <a href="https://arxiv.org/abs/1708.03888">Large Batch Training of Convolutional Networks</a></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn2"><p><code>micro_batch_size</code> = batch_size <strong>per</strong> GPU</p></li></ol></aside></section>

<section id="recent-progress" class="title-slide slide level1 center">
<h1>Recent Progress</h1>
<div id="tbl-batch-scaling" class="striped hover quarto-float" style="font-size:0.7em; font-family: monospace;">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-batch-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Batch-Size-Scaling
</figcaption>
<div aria-describedby="tbl-batch-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover">
<colgroup>
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 21%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Year</th>
<th style="text-align: center;">Author</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;">Processor</th>
<th style="text-align: center;"># Processors</th>
<th style="text-align: center;">Time</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2016</td>
<td style="text-align: center;">He</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">Tesla P100</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">29 Hour</td>
<td style="text-align: center;">75.30%</td>
</tr>
<tr class="even">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Yamazaki</td>
<td style="text-align: center;">81,920</td>
<td style="text-align: center;">Tesla V100</td>
<td style="text-align: center;"><span class="red">2048</span></td>
<td style="text-align: center;"><span class="blue">1.2 Min</span></td>
<td style="text-align: center;">75.08%</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<!-- |                                 |             Goyal et al.             |               8192               |                 Tesla P100                  |             Caffe 2             |               1 Hour               |                            76.3%         | -->
<!-- |                                 |             Smith et al.             |         8192 ->  16,384          |                full TPU pod                 |           TensorFlow            |              30 Mins               |                            76.1%         | -->
<!-- |                                 |             Akiba et al.             |              32,768              |              Tesla P100 x1024               |             Chainer             |              15 Mins               |                            74.9%         | -->
<!-- |                                 |              Jia et al.              |              65,536              |              Tesla P40  x2048               |           TensorFLow            |              6.6 Mins              |                            75.8%         | -->
<!-- |                                 |             Ying et al.              |              65,536              |                TPU v3 x1024                 |           TensorFlow            |              1.8 Mins              |                            75.2%         | -->
<!-- |                                 |            Mikami et al.             |              55,296              |              Tesla V100 x3456               |               NNL               |              2.0 Mins              |                           75.29%         | -->
</section>

<section id="data-parallel-training" class="title-slide slide level1 center">
<h1>Data Parallel Training</h1>
<!-- ::: {layout="[40,-5, 50]" layout-valign="center" style="display: flex; align-items:flex-end;"} -->
<!---->
<!-- ::: {.col1} -->
<!---->
<!-- ::: -->

<!-- ::: -->
<img data-src="./assets/multi-gpu-ddp.drawio.svg" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;9
</p></section>

<section id="data-parallel-training-1" class="title-slide slide level1 center">
<h1>Data Parallel Training</h1>
<ul>
<li>üîó Links:
<ul>
<li><a href="https://pytorch.org/tutorials/beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li><a href="https://pytorch.org/docs/master/notes/ddp.html">Distributed Data Parallel ‚Äî PyTorch master documentation</a></li>
<li><a href="https://huggingface.co/docs/transformers/en/perf_train_gpu_many">ü§ó Efficient Training on Multiple GPUs</a></li>
<li><a href="https://www.deepspeed.ai/getting-started/">Getting Started - DeepSpeed</a></li>
</ul></li>
</ul>
</section>

<section id="data-parallel-training-2" class="title-slide slide level1 center">
<h1>Data Parallel Training</h1>

<img data-src="./assets/data-parallel-light.svg" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;10
</p></section>

<section id="data-parallel-training-3" class="title-slide slide level1 center">
<h1>Data Parallel Training</h1>
<div class="quarto-layout-panel" data-layout="[45,-5,50]" style="display: flex; align-items:flex-end;">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="col1 quarto-layout-cell" style="flex-basis: 45.0%;justify-content: flex-start;">
<ul>
<li>Typically easier to implement</li>
<li>Existing frameworks (<a href="https://horovod.readthedocs.io/en/stable/index.html">Horovod</a>, <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>, <a href="https://pytorch.org/docs/stable/notes/ddp.html">DDP</a>, etc)
<ul>
<li>Relatively simple to get up and running (minor modifications to code)
<ul>
<li><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/saforem2/ezpz"><code>saforem2/ezpz</code></a></li>
</ul></li>
</ul></li>
<li>Recent presentation on data-parallel training available on <a href="https://youtu.be/930yrXjNkgM">YouTube</a></li>
</ul>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="https://saforem2.github.io/distributed-training-slides/assets/data-parallel.svg" style="width:80.0%"></p>
</div>
</div>
</div>
</section>

<section id="data-parallel-training-4" class="title-slide slide level1 center">
<h1>Data Parallel Training</h1>
<div class="quarto-layout-panel" data-layout="[45,-5,50]" style="display: flex; align-items:flex-end;">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="col1 quarto-layout-cell" style="flex-basis: 45.0%;justify-content: flex-start;">
<ul>
<li>Each worker has <strong>copy of complete model</strong></li>
<li>Global batch of data split into multiple mini-batches
<ul>
<li>Each worker computes the corresponding <strong>loss and gradients from local data</strong></li>
</ul></li>
<li>Before updating parameters, loss and gradients averaged across workers</li>
</ul>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="https://saforem2.github.io/distributed-training-slides/assets/data-parallel.svg" style="width:80.0%"></p>
</div>
</div>
</div>
</section>

<section id="data-parallel-training-5" class="title-slide slide level1 center">
<h1>Data Parallel Training</h1>

<img data-src="https://saforem2.github.io/distributed-training-slides/assets/avgGrads.svg" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;11
</p></section>

<section id="deal-with-data" class="title-slide slide level1 center">
<h1>Deal with Data</h1>
<ul>
<li><p>At each training step, we want to ensure that <strong>each worker receives unique data</strong></p></li>
<li><p>This can be done in one of two ways:</p>
<ol type="1">
<li>Manually partition data (ahead of time) and assign different sections to different workers
<ol type="1">
<li>Each worker can only see their local portion of the data</li>
</ol></li>
<li>From each worker, randomly select a mini-batch
<ol type="1">
<li>Each worker can see the full dataset</li>
</ol></li>
</ol>
<div title="‚ö†Ô∏è  Warning">
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>‚ö†Ô∏è Warning</strong></p>
</div>
<div class="callout-content">
<p>Don‚Äôt forget your seed!</p>
<p>When randomly selecting, it is important that each worker uses different seeds to ensure they receive unique data</p>
</div>
</div>
</div>
</div></li>
</ul>
</section>

<section id="best-practices" class="title-slide slide level1 smaller center">
<h1>Best Practices</h1>
<ul>
<li>Use parallel IO whenever possible
<ul>
<li>Feed each rank from different files</li>
<li>Use MPI IO to have each rank read its own batch from a file</li>
<li>Use several ranks to read data, MPI to scatter to remaining ranks
<ul>
<li>Most practical in big <em>at-scale</em> training</li>
</ul></li>
</ul>
<div title="ü§ù Keeping things in Sync">
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>ü§ù Keeping things in Sync</strong></p>
</div>
<div class="callout-content">
<p><strong>Computation stalls during communication !!</strong></p>
<p>Keeping the communication to computation ratio small is important for effective scaling.</p>
</div>
</div>
</div>
</div></li>
<li>Take advantage of data storage
<ul>
<li>Use <a href="https://wiki.lustre.org/Configuring_Lustre_File_Striping">striping on lustre</a></li>
<li>Use the right optimizations for Aurora, Polaris, etc.</li>
</ul></li>
<li>Preload data when possible
<ul>
<li>Offloading to a GPU frees CPU cycles for loading the next batch of data
<ul>
<li><strong>minimize IO latency this way</strong></li>
</ul></li>
</ul></li>
</ul>
</section>

<section id="broadcast-initial-state" class="title-slide slide level1 center">
<h1>Broadcast Initial State</h1>
<ul>
<li>At the start of training (or when loading from a checkpoint), we want all of our workers to be initialized consistently
<ul>
<li><strong>Broadcast</strong> the model and optimizer states from <code>rank() == 0</code> worker</li>
</ul></li>
</ul>
<div class="cell" data-reveal="true" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">  flowchart TD
    0["GPU0"] --&gt; 1["GPU 1"]
    0 --&gt; 2["GPU 2"]
    0 --&gt;|Model + Optimizer State| 3["GPU 3"]
    0 --&gt; ...
    0 --&gt; N["GPU N"]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>

<section id="model-parallel-training" class="title-slide slide level1 center">
<h1>Model Parallel Training</h1>
<div class="quarto-layout-panel" data-layout="[60,40]" style="display: flex; align-items:flex-end;">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="col1 quarto-layout-cell" style="flex-basis: 60.0%;justify-content: flex-start;">
<ul>
<li><p>Split up network over multiple workers</p>
<ul>
<li>Each receives disjoint subset</li>
<li>All communication associated with subsets are distributed</li>
</ul></li>
<li><p>Communication whenever dataflow between two subsets</p></li>
<li><p>Typically <strong>more complicated</strong> to implement than data parallel training</p></li>
<li><p>Suitable when the model is too large to fit onto a single device (CPU / GPU)</p></li>
<li><p><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed"><code>argonne-lcf/Megatron-DeepSpeed</code></a></p></li>
<li><p>ü§ó <a href="https://github.com/huggingface/nanotron"><code>huggingface/nanotron</code></a></p></li>
</ul>
</div>
<div class="quarto-layout-cell" style="flex-basis: 40.0%;justify-content: flex-start;">
<div id="fig-model-parallel-1" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-model-parallel-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<!-- ![](./assets/model-parallelism.svg) -->
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-parallel-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <img data-src="https://saforem2.github.io/distributed-training-slides/assets/model-parallel.svg">
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>

<section id="model-parallel-training-example" class="title-slide slide level1 center">
<h1>Model Parallel Training: Example</h1>
<p><span class="math display">y = \sum_{i} w_{i} * x_{i} = w_0 * x_0 + w_1 * x_1 + w_2 * x_2</span></p>
<ol type="1">
<li>Compute <span class="math inline">y_{0} = w_{0} * x_{0}</span> and send to <span class="math inline">\longrightarrow</span> <code>GPU1</code></li>
<li>Compute <span class="math inline">y_{1} = y_{0} + w_{1} * x_{1}</span> and send to <span class="math inline">\longrightarrow</span> <code>GPU2</code></li>
<li>Compute <span class="math inline">y = y_{1} * w_{2} * x_{2}</span> ‚úÖ</li>
</ol>
<div class="cell" data-reveal="true" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  subgraph X0["GPU0"]
    direction LR
    a["w0"]
  end
  subgraph X1["GPU1"]
    direction LR
    b["w1"]
  end
  subgraph X2["GPU2"]
    direction LR
    c["w2"]
  end
  X1 &amp; X0 &lt;--&gt; X2
  X0 &lt;--&gt; X1
  x["x0, x1, x2"] --&gt; X0
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>

<section id="hands-on" class="title-slide slide level1 center">
<h1>Hands-On</h1>
<ul>
<li><p><a href="https://github.com/argonne-lcf/ai-science-training-series/blob/main/06_parallel_training/README.md">Instructions</a></p>
<ul>
<li><i class="fa-brands fa-github" aria-label="github"></i> <a href="https://saforem2.github.io/wordplay/"><code>saforem2/wordplay</code> üéÆüí¨</a> [<a href="https://saforem2.github.io/wordplay/">web</a>]</li>
</ul></li>
</ul>
</section>

<section id="thank-you" class="title-slide slide level1 center">
<h1>Thank you!</h1>
<ul>
<li><p>Organizers</p></li>
<li><p>ALCF Data Science &amp; Operations</p></li>
<li><p>Feel free to reach out! <split even=""></split></p>
<p><a href="https://samforeman.me"><i class="fas fa-home"></i></a> <a href="mailto:///foremans@anl.gov"><i class="far fa-paper-plane"></i></a> <a href="https://www.twitter.com/saforem2"><i class="fab fa-twitter"></i></a> </p></li>
</ul>
<div class="callout-info" data-icon="false" title="üôè Acknowledgements">
<p>This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.</p>
</div>
</section>

<section>
<section id="backups" class="title-slide slide level1 center">
<h1>Backups</h1>

</section>
<section id="forward-pass" class="slide level2 center">
<h2>Forward Pass</h2>
<ul>
<li>Each worker has identical copy of model</li>
<li><strong>Global batch of data split across workers</strong></li>
<li>Loss + Grads averaged across workers before updating parameters</li>
</ul>
<div class="cell" data-reveal="true" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">    flowchart TD
      D["dataset"] --&gt; S1["subset_1"]
      D --&gt; S2["subset_2"]
      D --&gt; S3["subset_3"]
      D --&gt; S4["subset_4"]
      S1 --&gt; W1["Worker 1"]
      S2 --&gt; W2["Worker 2"]
      S3 --&gt; W3["Worker 3"]
      S4 --&gt; W4["Worker 4"]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="organization" class="slide level2 center">
<h2>Organization</h2>
<div class="cell" data-reveal="true" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
  subgraph identifier[" "]
    direction LR
    GPU1
    GPU2
    GPU3
    GPU4
  end
  subgraph Network
    direction LR
    Model
  end
  Network -.-&gt; GPU1
  Network -.-&gt; GPU2
  Network -.-&gt; GPU3
  Network -.-&gt; GPU4
  subset1 --&gt; GPU1
  subset2 --&gt; GPU2
  subset3 --&gt; GPU3
  subset4 --&gt; GPU4
  subgraph Dataset
    direction LR
    subset1
    subset2
    subset3
    subset4
  end
  subgraph Communication
    direction LR
    GPU1 &lt;-.-&gt; AR[Allreduce]
    GPU2 &lt;-.-&gt; AR
    GPU3 &lt;-.-&gt; AR
    GPU4 &lt;-.-&gt; AR
  end
  AR ==&gt;|Broadcast| Network
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section></section>
<section>
<section id="data-parallel-training-6" class="title-slide slide level1 center">
<h1>Data Parallel Training</h1>
<div class="quarto-layout-panel" data-layout="[60,40]" style="display: flex; align-items:flex-end;">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="col1 quarto-layout-cell" style="flex-basis: 60.0%;justify-content: flex-start;">
<ul>
<li>Each worker receives <span class="red">identical</span> copy of <span class="red">model</span> and <span class="blue">unique</span> subset of <span class="blue">data</span></li>
</ul>
</div>
<div class="col2 quarto-layout-cell" style="flex-basis: 40.0%;justify-content: flex-start;">
<div class="cell" data-reveal="true" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    subgraph identifier[" "]
        direction LR
        data --&gt; subset1
        data --&gt; subset2
        data --&gt; subset3
        data --&gt; subset4
        subset1
        subset2
        subset3
        subset4
    end
    subgraph Workers
        direction LR
        subset1 --&gt; GPX1["GPU1"]
        subset2 --&gt; GPX2["GPU2"]
        subset3 --&gt; GPX3["GPU3"]
        subset4 --&gt; GPX4["GPU4"]
    end
    GPX1 &lt;.-&gt; Communication["Avg + Distribute Gradients"]
    GPX2 &lt;.-&gt; Communication
    GPX3 &lt;.-&gt; Communication
    GPX4 &lt;.-&gt; Communication
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="emergent-abilities" class="slide level2 center" data-background-color="#FBFBFD">
<h2>Emergent Abilities</h2>
<div width="66%" style="text-align: center;">
<p><img src="https://github.com/saforem2/llm-lunch-talk/blob/main/docs/assets/emergent-abilities.gif?raw=true" height="75%"></p>
<p><a href="https://arxiv.org/abs/2206.07682">Emergent abilities of Large Language Models</a> <span class="citation" data-cites="yao2023tree">Yao et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></p>
</div>
</section>
<section id="training-llms" class="slide level2 center">
<h2>Training LLMs</h2>
<div class="quarto-layout-panel" data-layout="[ 50, 40 ]">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: flex-start;">
<div id="fig-evolution" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="https://github.com/Mooler0410/LLMsPracticalGuide/raw/main/imgs/survey-gif-test.gif">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Visualization from <span class="citation" data-cites="yang2023harnessing">Yang et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 44.4%;justify-content: center;">
<p><img data-src="https://github.com/saforem2/llm-lunch-talk/blob/main/docs/assets/it_hungers.jpeg?raw=true"></p>
</div>
</div>
</div>
</section>
<section id="life-cycle-of-the-llm" class="slide level2 center" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Life-Cycle of the LLM</h2>
<div class="quarto-layout-panel" data-layout="[ 45, 55 ]">
<div class="quarto-layout-row quarto-layout-valign-center">
<div id="column-one" class="quarto-layout-cell" style="flex-basis: 45.0%;justify-content: flex-start;">
<ol type="1">
<li><p>Data collection + preprocessing</p></li>
<li><p><strong>Pre-training</strong></p>
<ul>
<li>Architecture decisions:<br>
<code>{model_size, hyperparameters,</code><br>
<code>parallelism, lr_schedule, ...}</code></li>
</ul></li>
<li><p>Supervised Fine-Tuning</p>
<ul>
<li>Instruction Tuning</li>
<li>Alignment</li>
</ul></li>
<li><p>Deploy (+ monitor, re-evaluate, etc.)</p></li>
</ol>
</div>
<div id="column-two" class="quarto-layout-cell" style="flex-basis: 55.0%;justify-content: flex-start;">
<div id="fig-pretrain-two" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-pretrain-two-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img data-src="https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif"></p>
<p><strong>Pre-training</strong>: Virtually all of the compute used during pretraining phase.</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pretrain-two-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Figure from <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="forward-pass-1" class="slide level2 center">
<h2>Forward Pass</h2>
<div id="fig-forward-pass" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-forward-pass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<video data-autoplay="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov">
</video>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-forward-pass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Language Model trained for causal language modeling. Video from: <a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial">ü§ó Generation with LLMs</a>
</figcaption>
</figure>
</div>
</section>
<section id="generating-text" class="slide level2 center">
<h2>Generating Text</h2>
<div id="fig-generating-text" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-generating-text-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<video data-autoplay="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov">
</video>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generating-text-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Language Model trained for causal language modeling. Video from: <a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial">ü§ó Generation with LLMs</a>
</figcaption>
</figure>
</div>
</section>
<section id="life-cycle-of-the-llm-pre-training" class="slide level2 center" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Life-Cycle of the LLM: Pre-training</h2>

<img data-src="https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;17: <strong>Pre-training</strong>: Virtually all of the compute used during pretraining phase
</p></section>
<section id="life-cycle-of-the-llm-fine-tuning" class="slide level2 center" data-auto-animate="true" style="font-size: 0.8em;">
<h2 data-id="quarto-animate-title">Life-Cycle of the LLM: Fine-Tuning</h2>
<div id="fig-pretrain-two" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-pretrain-two-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="https://jalammar.github.io/images/gpt3/10-gpt3-fine-tuning.gif">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pretrain-two-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: <strong>Fine-tuning</strong><sup>1</sup>: Fine-tuning actually updates the model‚Äôs weights to make the model better at a certain task.
</figcaption>
</figure>
</div>
<aside><ol class="aside-footnotes"><li id="fn3"><p>Figure from <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p></li></ol></aside></section>
<section id="assistant-models" class="slide level2 centeredslide center" data-background-color="#181D29">
<h2>Assistant Models</h2>

<img data-src="https://github.com/saforem2/LLM-tutorial/blob/main/docs/assets/jailbreak.jpeg?raw=true" class="r-stretch"></section>
<section id="saforem2wordplay" class="slide level2 center">
<h2><a href="https://github.com/saforem2/wordplay"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>saforem2/wordplay</code> üéÆüí¨</a></h2>
<!-- - [ `saforem2/wordplay`](https://github.com/saforem2/wordplay) -->
<ul>
<li>Fork of Andrej Karpathy‚Äôs <code>nanoGPT</code></li>
</ul>

<img data-src="https://github.com/saforem2/nanoGPT/raw/master/assets/nanogpt.jpg" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;19: The simplest, fastest repository for training / finetuning GPT based models.
</p></section>
<section id="saforem2wordplay-1" class="slide level2 center">
<h2><a href="https://github.com/saforem2/wordplay"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>saforem2/wordplay</code> üéÆüí¨</a></h2>
<div id="fig-compare" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-compare" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-nanogpt" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-subfloat-fig">
<div aria-describedby="fig-nanogpt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="https://github.com/saforem2/wordplay/blob/main/assets/car.png?raw=true" data-ref-parent="fig-compare" width="256">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-nanogpt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) <code>nanoGPT</code>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-compare" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-wordplay" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-subfloat-fig">
<div aria-describedby="fig-wordplay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="https://github.com/saforem2/wordplay/blob/main/assets/robot.png?raw=true" data-ref-parent="fig-compare" width="150">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wordplay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) <code>wordplay</code> üéÆ üí¨
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: <code>nanoGPT</code>, transformed.
</figcaption>
</figure>
</div>
</section>
<section id="install" class="slide level2 center">
<h2>Install</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install <span class="st">"git+https://github.com/saforem2/wordplay.git"</span></span>
<span id="cb1-2"><a aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-c</span> <span class="st">'import wordplay; print(wordplay.__file__)'</span></span>
<span id="cb1-3"><a aria-hidden="true" tabindex="-1"></a><span class="co"># ./wordplay/src/wordplay/__init__.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="dependencies" class="slide level2 center">
<h2>Dependencies</h2>
<ul>
<li><a href="https://github.com/huggingface/transformers"><code>transformers</code></a> for <iconify-icon inline="" icon="noto:hugging-face"></iconify-icon> transformers (to load <code>GPT-2</code> checkpoints)</li>
<li><a href="https://github.com/huggingface/datasets"><code>datasets</code></a> for <iconify-icon inline="" icon="noto:hugging-face"></iconify-icon> datasets (if you want to use OpenWebText)</li>
<li><a href="https://github.com/openai/tiktoken"><code>tiktoken</code></a> for OpenAI‚Äôs fast BPE code</li>
<li><a href="https://wandb.ai"><code>wandb</code></a> for optional logging</li>
<li><a href="https://github.com/tqdm/tqdm"><code>tqdm</code></a> for progress bars</li>
</ul>
</section>
<section id="quick-start" class="slide level2 center">
<h2>Quick Start</h2>
<ul>
<li><p>We start with training a character-level GPT on the works of Shakespeare.</p>
<ol type="1">
<li>Downloading the data (~ 1MB) file</li>
<li>Convert raw text to one large stream of integers</li>
</ol>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> data/shakespeare_char/prepare.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This will create <code>data/shakespeare_char/{train.bin, val.bin}</code>.</p></li>
</ul>
</section>
<section id="model-model.py" class="slide level2 center" height="100%">
<h2>Model <a href="https://github.com/saforem2/wordplay/blob/master/src/wordplay/model.py"><iconify-icon inline="" icon="fa-brands:github"></iconify-icon> <code>model.py</code></a></h2>
<div class="panel-tabset" style="height: 100%!important;">
<ul id="tabset-2" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-2-1"><code>CausalSelfAttention</code></a></li><li><a href="#tabset-2-2"><code>LayerNorm</code></a></li><li><a href="#tabset-2-3"><code>MLP</code></a></li><li><a href="#tabset-2-4"><code>Block</code></a></li><li><a href="#tabset-2-5"><code>GPT</code></a></li></ul>
<div class="tab-content" style="height: 100%!important;">
<div id="tabset-2-1">
<div class="sourceCode" id="cb3" data-startfrom="65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 64;"><span id="cb3-65"><a></a><span class="kw">class</span> CausalSelfAttention(nn.Module):</span>
<span id="cb3-66"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: GPTModelConfig):</span>
<span id="cb3-67"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-68"><a></a>        <span class="cf">assert</span> config.n_embd <span class="op">%</span> config.n_head <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb3-69"><a></a>        <span class="co"># key, query, value projections for all heads, but in a batch</span></span>
<span id="cb3-70"><a></a>        <span class="va">self</span>.c_attn <span class="op">=</span> nn.Linear(</span>
<span id="cb3-71"><a></a>            config.n_embd,</span>
<span id="cb3-72"><a></a>            <span class="dv">3</span> <span class="op">*</span> config.n_embd,</span>
<span id="cb3-73"><a></a>            bias<span class="op">=</span>config.bias</span>
<span id="cb3-74"><a></a>        )</span>
<span id="cb3-75"><a></a>        <span class="co"># output projection</span></span>
<span id="cb3-76"><a></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(</span>
<span id="cb3-77"><a></a>            config.n_embd,</span>
<span id="cb3-78"><a></a>            config.n_embd,</span>
<span id="cb3-79"><a></a>            bias<span class="op">=</span>config.bias</span>
<span id="cb3-80"><a></a>        )</span>
<span id="cb3-81"><a></a>        <span class="co"># regularization</span></span>
<span id="cb3-82"><a></a>        <span class="va">self</span>.attn_dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb3-83"><a></a>        <span class="va">self</span>.resid_dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb3-84"><a></a>        <span class="va">self</span>.n_head <span class="op">=</span> config.n_head</span>
<span id="cb3-85"><a></a>        <span class="va">self</span>.n_embd <span class="op">=</span> config.n_embd</span>
<span id="cb3-86"><a></a>        <span class="va">self</span>.dropout <span class="op">=</span> config.dropout</span>
<span id="cb3-87"><a></a>        <span class="co"># flash attention make GPU go brrrrr but support is only in</span></span>
<span id="cb3-88"><a></a>        <span class="co"># PyTorch &gt;= 2.0</span></span>
<span id="cb3-89"><a></a>        <span class="va">self</span>.flash <span class="op">=</span> <span class="bu">hasattr</span>(</span>
<span id="cb3-90"><a></a>            torch.nn.functional,</span>
<span id="cb3-91"><a></a>            <span class="st">'scaled_dot_product_attention'</span></span>
<span id="cb3-92"><a></a>        )</span>
<span id="cb3-93"><a></a>        <span class="co"># if self.flash and RANK == 0:</span></span>
<span id="cb3-94"><a></a>        <span class="co">#     log.warning(</span></span>
<span id="cb3-95"><a></a>        <span class="co">#         f'Using torch.nn.functional.scaled_dot_product_attention'</span></span>
<span id="cb3-96"><a></a>        <span class="co">#         '(Flash Attn)'</span></span>
<span id="cb3-97"><a></a>        <span class="co">#     )</span></span>
<span id="cb3-98"><a></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.flash:</span>
<span id="cb3-99"><a></a>            log.warning(</span>
<span id="cb3-100"><a></a>                <span class="st">"WARNING: using slow attention."</span></span>
<span id="cb3-101"><a></a>                <span class="st">"Flash Attention requires PyTorch &gt;= 2.0"</span></span>
<span id="cb3-102"><a></a>            )</span>
<span id="cb3-103"><a></a>            <span class="co"># causal mask to ensure that attention is only applied to the left</span></span>
<span id="cb3-104"><a></a>            <span class="co"># in the input sequence</span></span>
<span id="cb3-105"><a></a>            <span class="va">self</span>.register_buffer(</span>
<span id="cb3-106"><a></a>                <span class="st">"bias"</span>,</span>
<span id="cb3-107"><a></a>                torch.tril(</span>
<span id="cb3-108"><a></a>                    torch.ones(</span>
<span id="cb3-109"><a></a>                        config.block_size,</span>
<span id="cb3-110"><a></a>                        config.block_size</span>
<span id="cb3-111"><a></a>                    )</span>
<span id="cb3-112"><a></a>                ).view(<span class="dv">1</span>, <span class="dv">1</span>, config.block_size, config.block_size)</span>
<span id="cb3-113"><a></a>            )</span>
<span id="cb3-114"><a></a></span>
<span id="cb3-115"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-116"><a></a>        <span class="co"># batch size, sequence length, embedding dimensionality (n_embd)</span></span>
<span id="cb3-117"><a></a>        B, T, C <span class="op">=</span> x.size()</span>
<span id="cb3-118"><a></a></span>
<span id="cb3-119"><a></a>        <span class="co"># calculate query, key, values for all heads in batch and move head</span></span>
<span id="cb3-120"><a></a>        <span class="co"># forward to be the batch dim</span></span>
<span id="cb3-121"><a></a>        q, k, v <span class="op">=</span> <span class="va">self</span>.c_attn(x).split(<span class="va">self</span>.n_embd, dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-122"><a></a>        <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb3-123"><a></a>        k <span class="op">=</span> k.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-124"><a></a>        <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb3-125"><a></a>        q <span class="op">=</span> q.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-126"><a></a>        <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb3-127"><a></a>        v <span class="op">=</span> v.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-128"><a></a>        <span class="co"># causal self-attention; Self-attend:</span></span>
<span id="cb3-129"><a></a>        <span class="co"># (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span></span>
<span id="cb3-130"><a></a>        <span class="cf">if</span> <span class="va">self</span>.flash:</span>
<span id="cb3-131"><a></a>            <span class="co"># efficient attention using Flash Attention CUDA kernels</span></span>
<span id="cb3-132"><a></a>            y <span class="op">=</span> torch.nn.functional.scaled_dot_product_attention(</span>
<span id="cb3-133"><a></a>                q,</span>
<span id="cb3-134"><a></a>                k,</span>
<span id="cb3-135"><a></a>                v,</span>
<span id="cb3-136"><a></a>                attn_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb3-137"><a></a>                dropout_p<span class="op">=</span>(<span class="va">self</span>.dropout <span class="cf">if</span> <span class="va">self</span>.training <span class="cf">else</span> <span class="dv">0</span>),</span>
<span id="cb3-138"><a></a>                is_causal<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-139"><a></a>            )</span>
<span id="cb3-140"><a></a>        <span class="cf">else</span>:</span>
<span id="cb3-141"><a></a>            <span class="co"># manual implementation of attention</span></span>
<span id="cb3-142"><a></a>            att <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> (<span class="fl">1.0</span> <span class="op">/</span> math.sqrt(k.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb3-143"><a></a>            att <span class="op">=</span> att.masked_fill(</span>
<span id="cb3-144"><a></a>                <span class="va">self</span>.bias[:, :, :T, :T] <span class="op">==</span> <span class="dv">0</span>,  <span class="co"># type:ignore</span></span>
<span id="cb3-145"><a></a>                <span class="bu">float</span>(<span class="st">'-inf'</span>)</span>
<span id="cb3-146"><a></a>            )</span>
<span id="cb3-147"><a></a>            att <span class="op">=</span> F.softmax(att, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-148"><a></a>            att <span class="op">=</span> <span class="va">self</span>.attn_dropout(att)</span>
<span id="cb3-149"><a></a>            y <span class="op">=</span> att <span class="op">@</span> v  <span class="co"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span></span>
<span id="cb3-150"><a></a>        <span class="co"># re-assemble all head outputs side by side</span></span>
<span id="cb3-151"><a></a>        y <span class="op">=</span> y.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, T, C)</span>
<span id="cb3-152"><a></a></span>
<span id="cb3-153"><a></a>        <span class="co"># output projection</span></span>
<span id="cb3-154"><a></a>        y <span class="op">=</span> <span class="va">self</span>.resid_dropout(<span class="va">self</span>.c_proj(y))</span>
<span id="cb3-155"><a></a>        <span class="cf">return</span> y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-2">
<div class="sourceCode" id="cb4" data-startfrom="43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 42;"><span id="cb4-43"><a></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb4-44"><a></a>    <span class="co">"""</span></span>
<span id="cb4-45"><a></a><span class="co">    LayerNorm but with an optional bias.</span></span>
<span id="cb4-46"><a></a></span>
<span id="cb4-47"><a></a><span class="co">    (PyTorch doesn't support simply bias=False)</span></span>
<span id="cb4-48"><a></a><span class="co">    """</span></span>
<span id="cb4-49"><a></a></span>
<span id="cb4-50"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ndim, bias):</span>
<span id="cb4-51"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-52"><a></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.ones(ndim))</span>
<span id="cb4-53"><a></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(ndim)) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb4-54"><a></a></span>
<span id="cb4-55"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb4-56"><a></a>        <span class="cf">return</span> F.layer_norm(</span>
<span id="cb4-57"><a></a>            <span class="bu">input</span>,</span>
<span id="cb4-58"><a></a>            <span class="va">self</span>.weight.shape,</span>
<span id="cb4-59"><a></a>            <span class="va">self</span>.weight,</span>
<span id="cb4-60"><a></a>            <span class="va">self</span>.bias,</span>
<span id="cb4-61"><a></a>            <span class="fl">1e-5</span></span>
<span id="cb4-62"><a></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-3">
<div class="sourceCode" id="cb5" data-startfrom="165"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 164;"><span id="cb5-165"><a></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb5-166"><a></a></span>
<span id="cb5-167"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb5-168"><a></a>            <span class="va">self</span>,</span>
<span id="cb5-169"><a></a>            config: GPTModelConfig,</span>
<span id="cb5-170"><a></a>            activation: <span class="bu">str</span> <span class="op">=</span> <span class="st">'gelu'</span>,</span>
<span id="cb5-171"><a></a>    ):</span>
<span id="cb5-172"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-173"><a></a>        <span class="va">self</span>.c_fc <span class="op">=</span> nn.Linear(</span>
<span id="cb5-174"><a></a>            config.n_embd,</span>
<span id="cb5-175"><a></a>            <span class="dv">4</span> <span class="op">*</span> config.n_embd,</span>
<span id="cb5-176"><a></a>            bias<span class="op">=</span>config.bias</span>
<span id="cb5-177"><a></a>        )</span>
<span id="cb5-178"><a></a>        <span class="cf">if</span> activation.lower() <span class="kw">in</span> ACTIVATIONS:</span>
<span id="cb5-179"><a></a>            <span class="va">self</span>.act_fn <span class="op">=</span> ACTIVATIONS[activation.lower()]</span>
<span id="cb5-180"><a></a>        <span class="cf">else</span>:</span>
<span id="cb5-181"><a></a>            <span class="cf">try</span>:</span>
<span id="cb5-182"><a></a>                act_fn <span class="op">=</span> <span class="bu">getattr</span>(nn, activation)</span>
<span id="cb5-183"><a></a>                <span class="cf">assert</span> <span class="bu">callable</span>(act_fn)</span>
<span id="cb5-184"><a></a>                <span class="va">self</span>.act_fn <span class="op">=</span> act_fn()</span>
<span id="cb5-185"><a></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> exc:</span>
<span id="cb5-186"><a></a>                log.error(<span class="ss">f'</span><span class="sc">{</span>activation<span class="sc">}</span><span class="ss"> not yet supported!'</span>)</span>
<span id="cb5-187"><a></a>                <span class="cf">raise</span> exc</span>
<span id="cb5-188"><a></a>        <span class="co"># self.gelu = nn.GELU()</span></span>
<span id="cb5-189"><a></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(</span>
<span id="cb5-190"><a></a>            <span class="dv">4</span> <span class="op">*</span> config.n_embd,</span>
<span id="cb5-191"><a></a>            config.n_embd,</span>
<span id="cb5-192"><a></a>            bias<span class="op">=</span>config.bias</span>
<span id="cb5-193"><a></a>        )</span>
<span id="cb5-194"><a></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb5-195"><a></a></span>
<span id="cb5-196"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-197"><a></a>        x <span class="op">=</span> <span class="va">self</span>.c_fc(x)</span>
<span id="cb5-198"><a></a>        <span class="co"># x = self.gelu(x)</span></span>
<span id="cb5-199"><a></a>        x <span class="op">=</span> <span class="va">self</span>.act_fn(x)</span>
<span id="cb5-200"><a></a>        x <span class="op">=</span> <span class="va">self</span>.c_proj(x)</span>
<span id="cb5-201"><a></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb5-202"><a></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-4">
<div class="sourceCode" id="cb6" data-startfrom="205"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 204;"><span id="cb6-205"><a></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb6-206"><a></a></span>
<span id="cb6-207"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: GPTModelConfig):</span>
<span id="cb6-208"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-209"><a></a>        <span class="va">self</span>.ln_1 <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb6-210"><a></a>        <span class="va">self</span>.attn <span class="op">=</span> CausalSelfAttention(config)</span>
<span id="cb6-211"><a></a>        <span class="va">self</span>.ln_2 <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb6-212"><a></a>        <span class="va">self</span>.mlp <span class="op">=</span> MLP(config)</span>
<span id="cb6-213"><a></a></span>
<span id="cb6-214"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-215"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln_1(x))</span>
<span id="cb6-216"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.ln_2(x))</span>
<span id="cb6-217"><a></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-5">
<div class="sourceCode" id="cb7" data-startfrom="220"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 219;"><span id="cb7-220"><a></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb7-221"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: GPTModelConfig):</span>
<span id="cb7-222"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-223"><a></a>        <span class="cf">assert</span> config.vocab_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb7-224"><a></a>        <span class="cf">assert</span> config.block_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb7-225"><a></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb7-226"><a></a></span>
<span id="cb7-227"><a></a>        <span class="va">self</span>.transformer <span class="op">=</span> nn.ModuleDict(<span class="bu">dict</span>(</span>
<span id="cb7-228"><a></a>            wte<span class="op">=</span>nn.Embedding(config.vocab_size, config.n_embd),</span>
<span id="cb7-229"><a></a>            wpe<span class="op">=</span>nn.Embedding(config.block_size, config.n_embd),</span>
<span id="cb7-230"><a></a>            drop<span class="op">=</span>nn.Dropout(config.dropout),</span>
<span id="cb7-231"><a></a>            h<span class="op">=</span>nn.ModuleList([Block(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.n_layer)]),</span>
<span id="cb7-232"><a></a>            ln_f<span class="op">=</span>LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias),</span>
<span id="cb7-233"><a></a>        ))</span>
<span id="cb7-234"><a></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(config.n_embd, config.vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-235"><a></a>        <span class="co"># with weight tying when using torch.compile() some warnings get</span></span>
<span id="cb7-236"><a></a>        <span class="co"># generated: "UserWarning: functional_call was passed multiple values</span></span>
<span id="cb7-237"><a></a>        <span class="co"># for tied weights. This behavior is deprecated and will be an error in</span></span>
<span id="cb7-238"><a></a>        <span class="co"># future versions" not 100% sure what this is, so far seems to be</span></span>
<span id="cb7-239"><a></a>        <span class="co"># harmless. </span><span class="al">TODO</span><span class="co"> investigate</span></span>
<span id="cb7-240"><a></a>        <span class="co"># https://paperswithcode.com/method/weight-tying</span></span>
<span id="cb7-241"><a></a>        <span class="va">self</span>.transformer.wte.weight <span class="op">=</span> <span class="va">self</span>.lm_head.weight  <span class="co"># type:ignore</span></span>
<span id="cb7-242"><a></a></span>
<span id="cb7-243"><a></a>        <span class="co"># init all weights</span></span>
<span id="cb7-244"><a></a>        <span class="va">self</span>.<span class="bu">apply</span>(<span class="va">self</span>._init_weights)</span>
<span id="cb7-245"><a></a>        <span class="co"># apply special scaled init to the residual projections, per GPT-2</span></span>
<span id="cb7-246"><a></a>        <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters():</span>
<span id="cb7-247"><a></a>            <span class="cf">if</span> pn.endswith(<span class="st">'c_proj.weight'</span>):</span>
<span id="cb7-248"><a></a>                torch.nn.init.normal_(</span>
<span id="cb7-249"><a></a>                    p,</span>
<span id="cb7-250"><a></a>                    mean<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb7-251"><a></a>                    std<span class="op">=</span><span class="fl">0.02</span><span class="op">/</span>math.sqrt(<span class="dv">2</span> <span class="op">*</span> config.n_layer)</span>
<span id="cb7-252"><a></a>                )</span>
<span id="cb7-253"><a></a></span>
<span id="cb7-254"><a></a>        <span class="co"># report number of parameters</span></span>
<span id="cb7-255"><a></a>        log.info(<span class="st">"number of parameters: </span><span class="sc">%.2f</span><span class="st">M"</span> <span class="op">%</span> (<span class="va">self</span>.get_num_params()<span class="op">/</span><span class="fl">1e6</span>,))</span>
<span id="cb7-256"><a></a></span>
<span id="cb7-257"><a></a>    <span class="kw">def</span> get_num_params(<span class="va">self</span>, non_embedding<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb7-258"><a></a>        <span class="co">"""</span></span>
<span id="cb7-259"><a></a><span class="co">        Return the number of parameters in the model.</span></span>
<span id="cb7-260"><a></a><span class="co">        For non-embedding count (default), the position embeddings get</span></span>
<span id="cb7-261"><a></a><span class="co">        subtracted.</span></span>
<span id="cb7-262"><a></a></span>
<span id="cb7-263"><a></a><span class="co">        The token embeddings would too, except due to the parameter sharing</span></span>
<span id="cb7-264"><a></a><span class="co">        these params are actually used as weights in the final layer, so we</span></span>
<span id="cb7-265"><a></a><span class="co">        include them.</span></span>
<span id="cb7-266"><a></a><span class="co">        """</span></span>
<span id="cb7-267"><a></a>        n_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters())</span>
<span id="cb7-268"><a></a>        <span class="cf">if</span> non_embedding:</span>
<span id="cb7-269"><a></a>            n_params <span class="op">-=</span> <span class="va">self</span>.transformer.wpe.weight.numel()  <span class="co"># type:ignore</span></span>
<span id="cb7-270"><a></a>        <span class="cf">return</span> n_params</span>
<span id="cb7-271"><a></a></span>
<span id="cb7-272"><a></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb7-273"><a></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb7-274"><a></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb7-275"><a></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-276"><a></a>                torch.nn.init.zeros_(module.bias)</span>
<span id="cb7-277"><a></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb7-278"><a></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb7-279"><a></a></span>
<span id="cb7-280"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-281"><a></a>        device <span class="op">=</span> idx.device</span>
<span id="cb7-282"><a></a>        b, t <span class="op">=</span> idx.size()</span>
<span id="cb7-283"><a></a>        <span class="cf">assert</span> t <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size, (</span>
<span id="cb7-284"><a></a>            <span class="ss">f"Cannot forward sequence of length </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">, "</span></span>
<span id="cb7-285"><a></a>            <span class="st">"block size is only </span><span class="sc">{self.config.block_size}</span><span class="st">"</span></span>
<span id="cb7-286"><a></a>        )</span>
<span id="cb7-287"><a></a>        pos <span class="op">=</span> torch.arange(</span>
<span id="cb7-288"><a></a>            <span class="dv">0</span>,</span>
<span id="cb7-289"><a></a>            t,</span>
<span id="cb7-290"><a></a>            dtype<span class="op">=</span>torch.<span class="bu">long</span>,</span>
<span id="cb7-291"><a></a>            device<span class="op">=</span>device</span>
<span id="cb7-292"><a></a>        )  <span class="co"># shape (t)</span></span>
<span id="cb7-293"><a></a></span>
<span id="cb7-294"><a></a>        <span class="co"># forward the GPT model itself</span></span>
<span id="cb7-295"><a></a>        <span class="co"># token embeddings of shape (b, t, n_embd)</span></span>
<span id="cb7-296"><a></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.transformer.wte(idx)  <span class="co"># type:ignore</span></span>
<span id="cb7-297"><a></a>        <span class="co"># position embeddings of shape (t, n_embd)</span></span>
<span id="cb7-298"><a></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.transformer.wpe(pos)  <span class="co"># type:ignore</span></span>
<span id="cb7-299"><a></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.drop(tok_emb <span class="op">+</span> pos_emb)  <span class="co"># type:ignore</span></span>
<span id="cb7-300"><a></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:  <span class="co"># type:ignore</span></span>
<span id="cb7-301"><a></a>            x <span class="op">=</span> block(x)</span>
<span id="cb7-302"><a></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.ln_f(x)  <span class="co"># type:ignore</span></span>
<span id="cb7-303"><a></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-304"><a></a>            <span class="co"># if we are given some desired targets also calculate the loss</span></span>
<span id="cb7-305"><a></a>            logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb7-306"><a></a>            loss <span class="op">=</span> F.cross_entropy(</span>
<span id="cb7-307"><a></a>                logits.view(</span>
<span id="cb7-308"><a></a>                    <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb7-309"><a></a>                    logits.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-310"><a></a>                ),</span>
<span id="cb7-311"><a></a>                targets.view(<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb7-312"><a></a>                ignore_index<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb7-313"><a></a>            )</span>
<span id="cb7-314"><a></a>        <span class="cf">else</span>:</span>
<span id="cb7-315"><a></a>            <span class="co"># inference-time mini-optimization: only forward the lm_head on the</span></span>
<span id="cb7-316"><a></a>            <span class="co"># very last position</span></span>
<span id="cb7-317"><a></a>            <span class="co"># note: using list [-1] to preserve the time dim</span></span>
<span id="cb7-318"><a></a>            logits <span class="op">=</span> <span class="va">self</span>.lm_head(x[:, [<span class="op">-</span><span class="dv">1</span>], :])</span>
<span id="cb7-319"><a></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-320"><a></a></span>
<span id="cb7-321"><a></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb7-322"><a></a></span>
<span id="cb7-323"><a></a>    <span class="kw">def</span> crop_block_size(<span class="va">self</span>, block_size):</span>
<span id="cb7-324"><a></a>        <span class="co"># model surgery to decrease the block size if necessary e.g. we may</span></span>
<span id="cb7-325"><a></a>        <span class="co"># load the GPT2 pretrained model checkpoint (block size 1024) but want</span></span>
<span id="cb7-326"><a></a>        <span class="co"># to use a smaller block size for some smaller, simpler model</span></span>
<span id="cb7-327"><a></a>        <span class="cf">assert</span> block_size <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size</span>
<span id="cb7-328"><a></a>        <span class="va">self</span>.config.block_size <span class="op">=</span> block_size</span>
<span id="cb7-329"><a></a>        <span class="va">self</span>.transformer.wpe.weight <span class="op">=</span> (  <span class="co"># type:ignore</span></span>
<span id="cb7-330"><a></a>            nn.Parameter(</span>
<span id="cb7-331"><a></a>                <span class="va">self</span>.transformer.wpe.weight[:block_size]  <span class="co"># type:ignore</span></span>
<span id="cb7-332"><a></a>            )</span>
<span id="cb7-333"><a></a>        )</span>
<span id="cb7-334"><a></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:   <span class="co"># type:ignore</span></span>
<span id="cb7-335"><a></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(block.attn, <span class="st">'bias'</span>):</span>
<span id="cb7-336"><a></a>                block.attn.bias <span class="op">=</span> (</span>
<span id="cb7-337"><a></a>                    block.attn.bias[:, :, :block_size, :block_size]</span>
<span id="cb7-338"><a></a>                )</span>
<span id="cb7-339"><a></a></span>
<span id="cb7-340"><a></a>    <span class="at">@classmethod</span></span>
<span id="cb7-341"><a></a>    <span class="kw">def</span> from_pretrained(cls, model_type, override_args<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-342"><a></a>        <span class="cf">assert</span> model_type <span class="kw">in</span> {<span class="st">'gpt2'</span>, <span class="st">'gpt2-medium'</span>, <span class="st">'gpt2-large'</span>, <span class="st">'gpt2-xl'</span>}</span>
<span id="cb7-343"><a></a>        override_args <span class="op">=</span> override_args <span class="kw">or</span> {}  <span class="co"># default to empty dict</span></span>
<span id="cb7-344"><a></a>        <span class="co"># only dropout can be overridden see more notes below</span></span>
<span id="cb7-345"><a></a>        <span class="cf">assert</span> <span class="bu">all</span>(k <span class="op">==</span> <span class="st">'dropout'</span> <span class="cf">for</span> k <span class="kw">in</span> override_args)</span>
<span id="cb7-346"><a></a>        <span class="im">from</span> transformers <span class="im">import</span> GPT2LMHeadModel</span>
<span id="cb7-347"><a></a>        log.info(<span class="ss">f"loading weights from pretrained gpt: </span><span class="sc">{</span>model_type<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-348"><a></a>        <span class="co"># n_layer, n_head and n_embd are determined from model_type</span></span>
<span id="cb7-349"><a></a>        <span class="co"># gpt2: 124M params</span></span>
<span id="cb7-350"><a></a>        <span class="co"># gpt2-medium: 350M params</span></span>
<span id="cb7-351"><a></a>        <span class="co"># gpt2-large: 774M params</span></span>
<span id="cb7-352"><a></a>        <span class="co"># gpt2-xl: 1558M params</span></span>
<span id="cb7-353"><a></a>        config_args <span class="op">=</span> {</span>
<span id="cb7-354"><a></a>            <span class="co"># 'baby-llama2': dict(n_layer=16, n_head=16, n_embed=1024),</span></span>
<span id="cb7-355"><a></a>            <span class="co"># 'llama2-7b': dict(n_layer=32, n_head=32, n_embd=4096),</span></span>
<span id="cb7-356"><a></a>            <span class="st">'gpt2'</span>: <span class="bu">dict</span>(n_layer<span class="op">=</span><span class="dv">12</span>, n_head<span class="op">=</span><span class="dv">12</span>, n_embd<span class="op">=</span><span class="dv">768</span>),</span>
<span id="cb7-357"><a></a>            <span class="st">'gpt2-medium'</span>: <span class="bu">dict</span>(n_layer<span class="op">=</span><span class="dv">24</span>, n_head<span class="op">=</span><span class="dv">16</span>, n_embd<span class="op">=</span><span class="dv">1024</span>),</span>
<span id="cb7-358"><a></a>            <span class="st">'gpt2-large'</span>: <span class="bu">dict</span>(n_layer<span class="op">=</span><span class="dv">36</span>, n_head<span class="op">=</span><span class="dv">20</span>, n_embd<span class="op">=</span><span class="dv">1280</span>),</span>
<span id="cb7-359"><a></a>            <span class="st">'gpt2-xl'</span>: <span class="bu">dict</span>(n_layer<span class="op">=</span><span class="dv">48</span>, n_head<span class="op">=</span><span class="dv">25</span>, n_embd<span class="op">=</span><span class="dv">1600</span>),</span>
<span id="cb7-360"><a></a>        }[model_type]</span>
<span id="cb7-361"><a></a>        <span class="co"># we can override the dropout rate, if desired</span></span>
<span id="cb7-362"><a></a>        <span class="cf">if</span> <span class="st">'dropout'</span> <span class="kw">in</span> override_args:</span>
<span id="cb7-363"><a></a>            log.info(<span class="ss">f"overriding dropout rate to </span><span class="sc">{</span>override_args[<span class="st">'dropout'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-364"><a></a>            config_args[<span class="st">'dropout'</span>] <span class="op">=</span> override_args[<span class="st">'dropout'</span>]</span>
<span id="cb7-365"><a></a>        <span class="co"># create a from-scratch initialized minGPT model</span></span>
<span id="cb7-366"><a></a>        log.info(<span class="st">"forcing vocab_size=50257, block_size=1024, bias=True"</span>)</span>
<span id="cb7-367"><a></a>        config <span class="op">=</span> GPTModelConfig(</span>
<span id="cb7-368"><a></a>            <span class="op">**</span>config_args,</span>
<span id="cb7-369"><a></a>            block_size<span class="op">=</span><span class="dv">1024</span>,   <span class="co"># always 1024 for GPT model checkpoints</span></span>
<span id="cb7-370"><a></a>            vocab_size<span class="op">=</span><span class="dv">50257</span>,  <span class="co"># always 50257 for GPT model checkpoints</span></span>
<span id="cb7-371"><a></a>            bias<span class="op">=</span><span class="va">True</span>,         <span class="co"># always True for GPT model checkpoints</span></span>
<span id="cb7-372"><a></a>        )</span>
<span id="cb7-373"><a></a>        model <span class="op">=</span> GPT(config)</span>
<span id="cb7-374"><a></a>        sd <span class="op">=</span> model.state_dict()</span>
<span id="cb7-375"><a></a>        sd_keys <span class="op">=</span> sd.keys()</span>
<span id="cb7-376"><a></a>        sd_keys <span class="op">=</span> [</span>
<span id="cb7-377"><a></a>            k <span class="cf">for</span> k <span class="kw">in</span> sd_keys <span class="cf">if</span> <span class="kw">not</span> k.endswith(<span class="st">'.attn.bias'</span>)</span>
<span id="cb7-378"><a></a>        ]  <span class="co"># discard this mask / buffer, not a param</span></span>
<span id="cb7-379"><a></a></span>
<span id="cb7-380"><a></a>        <span class="co"># init a huggingface/transformers model</span></span>
<span id="cb7-381"><a></a>        model_hf <span class="op">=</span> GPT2LMHeadModel.from_pretrained(model_type)</span>
<span id="cb7-382"><a></a>        sd_hf <span class="op">=</span> model_hf.state_dict()</span>
<span id="cb7-383"><a></a></span>
<span id="cb7-384"><a></a>        <span class="co"># copy while ensuring all of the parameters are aligned and match in</span></span>
<span id="cb7-385"><a></a>        <span class="co"># names and shapes</span></span>
<span id="cb7-386"><a></a>        sd_keys_hf <span class="op">=</span> sd_hf.keys()</span>
<span id="cb7-387"><a></a>        sd_keys_hf <span class="op">=</span> [</span>
<span id="cb7-388"><a></a>            k <span class="cf">for</span> k <span class="kw">in</span> sd_keys_hf <span class="cf">if</span> <span class="kw">not</span> k.endswith(<span class="st">'.attn.masked_bias'</span>)</span>
<span id="cb7-389"><a></a>        ]  <span class="co"># ignore these, just a buffer</span></span>
<span id="cb7-390"><a></a>        sd_keys_hf <span class="op">=</span> [</span>
<span id="cb7-391"><a></a>            k <span class="cf">for</span> k <span class="kw">in</span> sd_keys_hf <span class="cf">if</span> <span class="kw">not</span> k.endswith(<span class="st">'.attn.bias'</span>)</span>
<span id="cb7-392"><a></a>        ]  <span class="co"># same, just the mask (buffer)</span></span>
<span id="cb7-393"><a></a>        transposed <span class="op">=</span> [</span>
<span id="cb7-394"><a></a>            <span class="st">'attn.c_attn.weight'</span>,</span>
<span id="cb7-395"><a></a>            <span class="st">'attn.c_proj.weight'</span>,</span>
<span id="cb7-396"><a></a>            <span class="st">'mlp.c_fc.weight'</span>,</span>
<span id="cb7-397"><a></a>            <span class="st">'mlp.c_proj.weight'</span></span>
<span id="cb7-398"><a></a>        ]</span>
<span id="cb7-399"><a></a>        <span class="co"># basically the openai checkpoints use a "Conv1D" module, but we only</span></span>
<span id="cb7-400"><a></a>        <span class="co"># want to use a vanilla Linear this means that we have to transpose</span></span>
<span id="cb7-401"><a></a>        <span class="co"># these weights when we import them</span></span>
<span id="cb7-402"><a></a>        <span class="cf">assert</span> <span class="bu">len</span>(sd_keys_hf) <span class="op">==</span> <span class="bu">len</span>(sd_keys), (</span>
<span id="cb7-403"><a></a>            <span class="ss">f"mismatched keys: </span><span class="sc">{</span><span class="bu">len</span>(sd_keys_hf)<span class="sc">}</span><span class="ss"> != </span><span class="sc">{</span><span class="bu">len</span>(sd_keys)<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb7-404"><a></a>        )</span>
<span id="cb7-405"><a></a>        <span class="cf">for</span> k <span class="kw">in</span> sd_keys_hf:</span>
<span id="cb7-406"><a></a>            <span class="cf">if</span> <span class="bu">any</span>(k.endswith(w) <span class="cf">for</span> w <span class="kw">in</span> transposed):</span>
<span id="cb7-407"><a></a>                <span class="co"># special treatment for the Conv1D weights we need to transpose</span></span>
<span id="cb7-408"><a></a>                <span class="cf">assert</span> sd_hf[k].shape[::<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> sd[k].shape</span>
<span id="cb7-409"><a></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-410"><a></a>                    sd[k].copy_(sd_hf[k].t())</span>
<span id="cb7-411"><a></a>            <span class="cf">else</span>:</span>
<span id="cb7-412"><a></a>                <span class="co"># vanilla copy over the other parameters</span></span>
<span id="cb7-413"><a></a>                <span class="cf">assert</span> sd_hf[k].shape <span class="op">==</span> sd[k].shape</span>
<span id="cb7-414"><a></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-415"><a></a>                    sd[k].copy_(sd_hf[k])</span>
<span id="cb7-416"><a></a></span>
<span id="cb7-417"><a></a>        <span class="cf">return</span> model</span>
<span id="cb7-418"><a></a></span>
<span id="cb7-419"><a></a>    <span class="kw">def</span> configure_optimizers(</span>
<span id="cb7-420"><a></a>            <span class="va">self</span>,</span>
<span id="cb7-421"><a></a>            weight_decay,</span>
<span id="cb7-422"><a></a>            learning_rate,</span>
<span id="cb7-423"><a></a>            betas,</span>
<span id="cb7-424"><a></a>            device_type</span>
<span id="cb7-425"><a></a>    ):</span>
<span id="cb7-426"><a></a>        <span class="co"># start with all of the candidate parameters</span></span>
<span id="cb7-427"><a></a>        <span class="co"># filter out those that do not require grad</span></span>
<span id="cb7-428"><a></a>        <span class="co"># param_dict = {</span></span>
<span id="cb7-429"><a></a>        <span class="co">#     pn: p for pn, p in param_dict.items() if p.requires_grad</span></span>
<span id="cb7-430"><a></a>        <span class="co"># }</span></span>
<span id="cb7-431"><a></a>        param_dict <span class="op">=</span> {</span>
<span id="cb7-432"><a></a>            pn: p <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters() <span class="cf">if</span> p.requires_grad</span>
<span id="cb7-433"><a></a>        }</span>
<span id="cb7-434"><a></a>        <span class="co"># create optim groups. Any parameters that is 2D will be weight</span></span>
<span id="cb7-435"><a></a>        <span class="co"># decayed, otherwise no. i.e. all weight tensors in matmuls +</span></span>
<span id="cb7-436"><a></a>        <span class="co"># embeddings decay, all biases and layernorms don't.</span></span>
<span id="cb7-437"><a></a>        decay_params <span class="op">=</span> [p <span class="cf">for</span> _, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.dim() <span class="op">&gt;=</span> <span class="dv">2</span>]</span>
<span id="cb7-438"><a></a>        nodecay_params <span class="op">=</span> [p <span class="cf">for</span> _, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.dim() <span class="op">&lt;</span> <span class="dv">2</span>]</span>
<span id="cb7-439"><a></a>        optim_groups <span class="op">=</span> [</span>
<span id="cb7-440"><a></a>            {<span class="st">'params'</span>: decay_params, <span class="st">'weight_decay'</span>: weight_decay},</span>
<span id="cb7-441"><a></a>            {<span class="st">'params'</span>: nodecay_params, <span class="st">'weight_decay'</span>: <span class="fl">0.0</span>}</span>
<span id="cb7-442"><a></a>        ]</span>
<span id="cb7-443"><a></a>        num_decay_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> decay_params)</span>
<span id="cb7-444"><a></a>        num_nodecay_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> nodecay_params)</span>
<span id="cb7-445"><a></a>        log.info(</span>
<span id="cb7-446"><a></a>            <span class="ss">f"num decayed parameter tensors: </span><span class="sc">{</span><span class="bu">len</span>(decay_params)<span class="sc">}</span><span class="ss">, "</span></span>
<span id="cb7-447"><a></a>            <span class="ss">f"with </span><span class="sc">{</span>num_decay_params<span class="sc">:,}</span><span class="ss"> parameters"</span></span>
<span id="cb7-448"><a></a>        )</span>
<span id="cb7-449"><a></a>        log.info(</span>
<span id="cb7-450"><a></a>            <span class="ss">f"num non-decayed parameter tensors: </span><span class="sc">{</span><span class="bu">len</span>(nodecay_params)<span class="sc">}</span><span class="ss">, "</span></span>
<span id="cb7-451"><a></a>            <span class="ss">f"with </span><span class="sc">{</span>num_nodecay_params<span class="sc">:,}</span><span class="ss"> parameters"</span></span>
<span id="cb7-452"><a></a>        )</span>
<span id="cb7-453"><a></a>        <span class="co"># Create AdamW optimizer and use the fused version if it is available</span></span>
<span id="cb7-454"><a></a>        fused_available <span class="op">=</span> (</span>
<span id="cb7-455"><a></a>            <span class="st">'fused'</span> <span class="kw">in</span> inspect.signature(torch.optim.AdamW).parameters</span>
<span id="cb7-456"><a></a>        )</span>
<span id="cb7-457"><a></a>        use_fused <span class="op">=</span> fused_available <span class="kw">and</span> device_type <span class="op">==</span> <span class="st">'cuda'</span></span>
<span id="cb7-458"><a></a>        extra_args <span class="op">=</span> <span class="bu">dict</span>(fused<span class="op">=</span><span class="va">True</span>) <span class="cf">if</span> use_fused <span class="cf">else</span> {}</span>
<span id="cb7-459"><a></a>        optimizer <span class="op">=</span> torch.optim.AdamW(</span>
<span id="cb7-460"><a></a>            optim_groups,</span>
<span id="cb7-461"><a></a>            lr<span class="op">=</span>learning_rate,</span>
<span id="cb7-462"><a></a>            betas<span class="op">=</span>betas,</span>
<span id="cb7-463"><a></a>            <span class="op">**</span>extra_args</span>
<span id="cb7-464"><a></a>        )</span>
<span id="cb7-465"><a></a>        log.info(<span class="ss">f"using fused AdamW: </span><span class="sc">{</span>use_fused<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-466"><a></a></span>
<span id="cb7-467"><a></a>        <span class="cf">return</span> optimizer</span>
<span id="cb7-468"><a></a></span>
<span id="cb7-469"><a></a>    <span class="kw">def</span> estimate_mfu(<span class="va">self</span>, fwdbwd_per_iter, dt):</span>
<span id="cb7-470"><a></a>        <span class="co">"""Estimate model flops utilization (MFU)</span></span>
<span id="cb7-471"><a></a></span>
<span id="cb7-472"><a></a><span class="co">        (in units of A100 bfloat16 peak FLOPS)</span></span>
<span id="cb7-473"><a></a><span class="co">        """</span></span>
<span id="cb7-474"><a></a>        <span class="co"># first estimate the number of flops we do per iteration.</span></span>
<span id="cb7-475"><a></a>        <span class="co"># see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311</span></span>
<span id="cb7-476"><a></a>        N <span class="op">=</span> <span class="va">self</span>.get_num_params()</span>
<span id="cb7-477"><a></a>        cfg <span class="op">=</span> <span class="va">self</span>.config</span>
<span id="cb7-478"><a></a>        L, H, Q, T <span class="op">=</span> (</span>
<span id="cb7-479"><a></a>            cfg.n_layer,</span>
<span id="cb7-480"><a></a>            cfg.n_head,</span>
<span id="cb7-481"><a></a>            cfg.n_embd<span class="op">//</span>cfg.n_head,</span>
<span id="cb7-482"><a></a>            cfg.block_size</span>
<span id="cb7-483"><a></a>        )</span>
<span id="cb7-484"><a></a>        flops_per_token <span class="op">=</span> <span class="dv">6</span><span class="op">*</span>N <span class="op">+</span> <span class="dv">12</span><span class="op">*</span>L<span class="op">*</span>H<span class="op">*</span>Q<span class="op">*</span>T</span>
<span id="cb7-485"><a></a>        flops_per_fwdbwd <span class="op">=</span> flops_per_token <span class="op">*</span> T</span>
<span id="cb7-486"><a></a>        flops_per_iter <span class="op">=</span> flops_per_fwdbwd <span class="op">*</span> fwdbwd_per_iter</span>
<span id="cb7-487"><a></a>        <span class="co"># express our flops throughput as ratio of A100 bfloat16 peak flops</span></span>
<span id="cb7-488"><a></a>        flops_achieved <span class="op">=</span> flops_per_iter <span class="op">*</span> (<span class="fl">1.0</span><span class="op">/</span>dt)  <span class="co"># per second</span></span>
<span id="cb7-489"><a></a>        flops_promised <span class="op">=</span> <span class="fl">312e12</span>  <span class="co"># A100 GPU bfloat16 peak flops is 312 TFLOPS</span></span>
<span id="cb7-490"><a></a>        <span class="cf">return</span> flops_achieved <span class="op">/</span> flops_promised</span>
<span id="cb7-491"><a></a></span>
<span id="cb7-492"><a></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb7-493"><a></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens, temperature<span class="op">=</span><span class="fl">1.0</span>, top_k<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-494"><a></a>        <span class="co">"""</span></span>
<span id="cb7-495"><a></a><span class="co">        Take a conditioning sequence of indices idx (LongTensor of shape (b,t))</span></span>
<span id="cb7-496"><a></a><span class="co">        and complete the sequence max_new_tokens times, feeding the predictions</span></span>
<span id="cb7-497"><a></a><span class="co">        back into the model each time.</span></span>
<span id="cb7-498"><a></a></span>
<span id="cb7-499"><a></a><span class="co">        Most likely you'll want to make sure to be in model.eval() mode of</span></span>
<span id="cb7-500"><a></a><span class="co">        operation for this.</span></span>
<span id="cb7-501"><a></a><span class="co">        """</span></span>
<span id="cb7-502"><a></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb7-503"><a></a>            <span class="co"># if the sequence context is growing too long we must crop it at</span></span>
<span id="cb7-504"><a></a>            <span class="co"># block_size</span></span>
<span id="cb7-505"><a></a>            idx_cond <span class="op">=</span> (</span>
<span id="cb7-506"><a></a>                idx <span class="cf">if</span> idx.size(<span class="dv">1</span>) <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size</span>
<span id="cb7-507"><a></a>                <span class="cf">else</span> idx[:, <span class="op">-</span><span class="va">self</span>.config.block_size:]</span>
<span id="cb7-508"><a></a>            )</span>
<span id="cb7-509"><a></a>            <span class="co"># forward the model to get the logits for the index in the sequence</span></span>
<span id="cb7-510"><a></a>            logits, _ <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb7-511"><a></a>            <span class="co"># pluck the logits at the final step and scale by desired</span></span>
<span id="cb7-512"><a></a>            <span class="co"># temperature</span></span>
<span id="cb7-513"><a></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="op">/</span> temperature</span>
<span id="cb7-514"><a></a>            <span class="co"># optionally crop the logits to only the top k options</span></span>
<span id="cb7-515"><a></a>            <span class="cf">if</span> top_k <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-516"><a></a>                v, _ <span class="op">=</span> torch.topk(logits, <span class="bu">min</span>(top_k, logits.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb7-517"><a></a>                logits[logits <span class="op">&lt;</span> v[:, [<span class="op">-</span><span class="dv">1</span>]]] <span class="op">=</span> <span class="op">-</span><span class="bu">float</span>(<span class="st">'Inf'</span>)</span>
<span id="cb7-518"><a></a>            <span class="co"># apply softmax to convert logits to (normalized) probabilities</span></span>
<span id="cb7-519"><a></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb7-520"><a></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb7-521"><a></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-522"><a></a>            <span class="co"># append sampled index to the running sequence and continue</span></span>
<span id="cb7-523"><a></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-524"><a></a></span>
<span id="cb7-525"><a></a>        <span class="cf">return</span> idx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="trainer-trainer.py" class="slide level2 center" height="100%">
<h2>Trainer <a href="https://github.com/saforem2/wordplay/blob/master/src/wordplay/trainer.py"><iconify-icon inline="" icon="fa-brands:github"></iconify-icon> <code>trainer.py</code></a></h2>
<div class="panel-tabset" style="font-size: 0.75em; width: 100%; height: 100%;">
<ul id="tabset-3" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-3-1"><code>get_batch</code></a></li><li><a href="#tabset-3-2"><code>_forward_step</code></a></li><li><a href="#tabset-3-3"><code>_backward_step</code></a></li><li><a href="#tabset-3-4"><code>train_step</code></a></li><li><a href="#tabset-3-5"><code>estimate_loss</code></a></li></ul>
<div class="tab-content" style="font-size: 0.75em; width: 100%; height: 100%;">
<div id="tabset-3-1">
<div class="sourceCode" id="cb8" data-startfrom="460"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 459;"><span id="cb8-460"><a></a>    <span class="kw">def</span> get_batch(<span class="va">self</span>, split: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[torch.Tensor, torch.Tensor]:</span>
<span id="cb8-461"><a></a>        <span class="co"># data = self.config.train_data if split == 'train'</span></span>
<span id="cb8-462"><a></a>        <span class="co"># else self.config.val_data</span></span>
<span id="cb8-463"><a></a>        data <span class="op">=</span> <span class="va">self</span>.config.data.data.get(split, <span class="va">None</span>)</span>
<span id="cb8-464"><a></a>        <span class="cf">assert</span> data <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb8-465"><a></a>        ix <span class="op">=</span> torch.randint(</span>
<span id="cb8-466"><a></a>            <span class="bu">len</span>(data) <span class="op">-</span> <span class="va">self</span>.config.model.block_size,</span>
<span id="cb8-467"><a></a>            (<span class="va">self</span>.config.model.batch_size,)</span>
<span id="cb8-468"><a></a>        )</span>
<span id="cb8-469"><a></a>        block_size <span class="op">=</span> <span class="va">self</span>.config.model.block_size</span>
<span id="cb8-470"><a></a>        x <span class="op">=</span> torch.stack(</span>
<span id="cb8-471"><a></a>            [</span>
<span id="cb8-472"><a></a>                torch.from_numpy((data[i:i<span class="op">+</span>block_size]).astype(np.int64))</span>
<span id="cb8-473"><a></a>                <span class="cf">for</span> i <span class="kw">in</span> ix</span>
<span id="cb8-474"><a></a>            ]</span>
<span id="cb8-475"><a></a>        )</span>
<span id="cb8-476"><a></a>        y <span class="op">=</span> torch.stack(</span>
<span id="cb8-477"><a></a>            [</span>
<span id="cb8-478"><a></a>                torch.from_numpy((data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span><span class="dv">1</span><span class="op">+</span>block_size]).astype(np.int64))</span>
<span id="cb8-479"><a></a>                <span class="cf">for</span> i <span class="kw">in</span> ix</span>
<span id="cb8-480"><a></a>            ]</span>
<span id="cb8-481"><a></a>        )</span>
<span id="cb8-482"><a></a>        <span class="cf">if</span> <span class="va">self</span>.config.device_type <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb8-483"><a></a>            x <span class="op">=</span> x.pin_memory().to(<span class="va">self</span>.config.device_type, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-484"><a></a>            y <span class="op">=</span> y.pin_memory().to(<span class="va">self</span>.config.device_type, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-485"><a></a>        <span class="cf">else</span>:</span>
<span id="cb8-486"><a></a>            x <span class="op">=</span> x.to(<span class="va">self</span>.config.device_type)</span>
<span id="cb8-487"><a></a>            y <span class="op">=</span> y.to(<span class="va">self</span>.config.device_type)</span>
<span id="cb8-488"><a></a>        <span class="cf">return</span> x, y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-3-2">
<div class="sourceCode" id="cb9" data-startfrom="548"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 547;"><span id="cb9-548"><a></a>    <span class="kw">def</span> _forward_step(<span class="va">self</span>, x: torch.Tensor, y: torch.Tensor) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb9-549"><a></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb9-550"><a></a>        <span class="cf">with</span> <span class="va">self</span>.config.ctx:</span>
<span id="cb9-551"><a></a>            logits, loss <span class="op">=</span> <span class="va">self</span>.model_engine(x, y)</span>
<span id="cb9-552"><a></a>        <span class="cf">return</span> {</span>
<span id="cb9-553"><a></a>            <span class="st">'logits'</span>: logits,</span>
<span id="cb9-554"><a></a>            <span class="st">'loss'</span>: loss,</span>
<span id="cb9-555"><a></a>            <span class="st">'dt'</span>: time.perf_counter() <span class="op">-</span> t0</span>
<span id="cb9-556"><a></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-3-3">
<div class="sourceCode" id="cb10" data-startfrom="558"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 557;"><span id="cb10-558"><a></a>    <span class="kw">def</span> _backward_step(</span>
<span id="cb10-559"><a></a>            <span class="va">self</span>,</span>
<span id="cb10-560"><a></a>            loss: torch.Tensor,</span>
<span id="cb10-561"><a></a>            propagate_grads: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb10-562"><a></a>    ) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb10-563"><a></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb10-564"><a></a>        <span class="cf">if</span> <span class="va">self</span>.config.train.backend.lower() <span class="kw">in</span> [<span class="st">'ds'</span>, <span class="st">'deepspeed'</span>]:</span>
<span id="cb10-565"><a></a>            <span class="va">self</span>.model_engine.backward(loss)  <span class="co"># type:ignore</span></span>
<span id="cb10-566"><a></a>            <span class="va">self</span>.model_engine.step(loss)      <span class="co"># type:ignore</span></span>
<span id="cb10-567"><a></a>        <span class="cf">else</span>:</span>
<span id="cb10-568"><a></a>            <span class="cf">if</span> <span class="va">self</span>.grad_scaler <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-569"><a></a>                <span class="va">self</span>.grad_scaler.scale(loss).backward()  <span class="co"># type:ignore</span></span>
<span id="cb10-570"><a></a>            <span class="cf">if</span> propagate_grads:</span>
<span id="cb10-571"><a></a>                <span class="cf">if</span> <span class="va">self</span>.config.optimizer.grad_clip <span class="op">!=</span> <span class="fl">0.0</span>:</span>
<span id="cb10-572"><a></a>                    <span class="cf">if</span> <span class="va">self</span>.grad_scaler <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-573"><a></a>                        <span class="va">self</span>.grad_scaler.unscale_(<span class="va">self</span>.optimizer)</span>
<span id="cb10-574"><a></a>                    torch.nn.utils.clip_grad_norm_(  <span class="co"># pyright: ignore</span></span>
<span id="cb10-575"><a></a>                        <span class="va">self</span>.model_engine.parameters(),</span>
<span id="cb10-576"><a></a>                        <span class="va">self</span>.config.optimizer.grad_clip</span>
<span id="cb10-577"><a></a>                    )</span>
<span id="cb10-578"><a></a>                <span class="cf">if</span> <span class="va">self</span>.grad_scaler <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-579"><a></a>                    <span class="va">self</span>.grad_scaler.step(<span class="va">self</span>.optimizer)</span>
<span id="cb10-580"><a></a>                    <span class="va">self</span>.grad_scaler.update()</span>
<span id="cb10-581"><a></a>                    <span class="va">self</span>.optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-582"><a></a></span>
<span id="cb10-583"><a></a>        <span class="cf">return</span> time.perf_counter() <span class="op">-</span> t0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-3-4">
<div class="sourceCode" id="cb11" data-startfrom="585"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 584;"><span id="cb11-585"><a></a>    <span class="kw">def</span> train_step(</span>
<span id="cb11-586"><a></a>            <span class="va">self</span>,</span>
<span id="cb11-587"><a></a>            x: torch.Tensor,</span>
<span id="cb11-588"><a></a>            y: torch.Tensor,</span>
<span id="cb11-589"><a></a>    ) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb11-590"><a></a>        lr <span class="op">=</span> (</span>
<span id="cb11-591"><a></a>            <span class="va">self</span>.get_lr(<span class="va">self</span>.config.iter_num)</span>
<span id="cb11-592"><a></a>            <span class="cf">if</span> <span class="va">self</span>.config.optimizer.decay_lr</span>
<span id="cb11-593"><a></a>            <span class="cf">else</span> <span class="va">self</span>._lr</span>
<span id="cb11-594"><a></a>        )</span>
<span id="cb11-595"><a></a>        <span class="cf">for</span> param_group <span class="kw">in</span> <span class="va">self</span>.optimizer.param_groups:</span>
<span id="cb11-596"><a></a>            param_group[<span class="st">'lr'</span>] <span class="op">=</span> lr</span>
<span id="cb11-597"><a></a>        dtf <span class="op">=</span> []</span>
<span id="cb11-598"><a></a>        dtb <span class="op">=</span> []</span>
<span id="cb11-599"><a></a>        dt <span class="op">=</span> []</span>
<span id="cb11-600"><a></a>        loss <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)</span>
<span id="cb11-601"><a></a>        <span class="cf">for</span> micro_step <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>._gas):</span>
<span id="cb11-602"><a></a>            is_last_micro_step <span class="op">=</span> (micro_step <span class="op">==</span> <span class="va">self</span>._gas <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb11-603"><a></a>            <span class="co"># </span><span class="al">NOTE</span><span class="co">: -----------------------------------------------------------</span></span>
<span id="cb11-604"><a></a>            <span class="co"># In DDP training we only need to sync gradients at the last micro</span></span>
<span id="cb11-605"><a></a>            <span class="co"># step. the official way to do this is with model.no_sync() context</span></span>
<span id="cb11-606"><a></a>            <span class="co"># manager, but I really dislike that this bloats the code and</span></span>
<span id="cb11-607"><a></a>            <span class="co"># forces us to repeat code looking at the source of that context</span></span>
<span id="cb11-608"><a></a>            <span class="co"># manager, it just toggles this variable</span></span>
<span id="cb11-609"><a></a>            <span class="co"># -----------------------------------------------------------------</span></span>
<span id="cb11-610"><a></a>            <span class="cf">if</span> <span class="va">self</span>.config.train.backend.lower() <span class="op">==</span> <span class="st">'ddp'</span>:</span>
<span id="cb11-611"><a></a>                _ <span class="op">=</span> (</span>
<span id="cb11-612"><a></a>                    <span class="va">self</span>.model_engine.require_backward_grad_sync</span>
<span id="cb11-613"><a></a>                    <span class="cf">if</span> (is_last_micro_step <span class="kw">and</span> <span class="va">self</span>.world_size <span class="op">&gt;</span> <span class="dv">1</span>)</span>
<span id="cb11-614"><a></a>                    <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb11-615"><a></a>                )</span>
<span id="cb11-616"><a></a>            fout <span class="op">=</span> <span class="va">self</span>._forward_step(x, y)</span>
<span id="cb11-617"><a></a>            <span class="co"># immediately async prefetch next batch while model is doing the</span></span>
<span id="cb11-618"><a></a>            <span class="co"># forward pass on the GPU</span></span>
<span id="cb11-619"><a></a>            x, y <span class="op">=</span> <span class="va">self</span>.get_batch(<span class="st">'train'</span>)</span>
<span id="cb11-620"><a></a>            loss <span class="op">=</span> fout[<span class="st">'loss'</span>] <span class="op">/</span> <span class="va">self</span>._gas</span>
<span id="cb11-621"><a></a>            dtf.append(fout[<span class="st">'dt'</span>])</span>
<span id="cb11-622"><a></a>            dtb_ <span class="op">=</span> <span class="va">self</span>._backward_step(</span>
<span id="cb11-623"><a></a>                loss,</span>
<span id="cb11-624"><a></a>                propagate_grads<span class="op">=</span>is_last_micro_step</span>
<span id="cb11-625"><a></a>            )</span>
<span id="cb11-626"><a></a>            dtb.append(dtb_)</span>
<span id="cb11-627"><a></a>            dt.append(dtf <span class="op">+</span> dtb)</span>
<span id="cb11-628"><a></a>        timers <span class="op">=</span> {</span>
<span id="cb11-629"><a></a>            <span class="st">'iter'</span>: <span class="va">self</span>.config.iter_num,</span>
<span id="cb11-630"><a></a>            <span class="st">'dt'</span>: np.array(dt),</span>
<span id="cb11-631"><a></a>            <span class="st">'dt_tot'</span>: np.<span class="bu">sum</span>(dt),</span>
<span id="cb11-632"><a></a>            <span class="st">'dt_avg'</span>: np.mean(dt),</span>
<span id="cb11-633"><a></a>            <span class="st">'dtf'</span>: np.array(dtf),</span>
<span id="cb11-634"><a></a>            <span class="st">'dtf_tot'</span>: np.<span class="bu">sum</span>(dtf),</span>
<span id="cb11-635"><a></a>            <span class="st">'dtf_avg'</span>: np.mean(dtf),</span>
<span id="cb11-636"><a></a>            <span class="st">'dtb'</span>: np.array(dtb),</span>
<span id="cb11-637"><a></a>            <span class="st">'dtb_tot'</span>: np.<span class="bu">sum</span>(dtb),</span>
<span id="cb11-638"><a></a>            <span class="st">'dtb_avg'</span>: np.mean(dtb)</span>
<span id="cb11-639"><a></a>        }</span>
<span id="cb11-640"><a></a>        metrics <span class="op">=</span> {</span>
<span id="cb11-641"><a></a>            <span class="st">'iter'</span>: <span class="va">self</span>.config.iter_num,</span>
<span id="cb11-642"><a></a>            <span class="st">'loss'</span>: loss,</span>
<span id="cb11-643"><a></a>            <span class="st">'lr'</span>: lr,</span>
<span id="cb11-644"><a></a>        }</span>
<span id="cb11-645"><a></a>        <span class="va">self</span>.config.iter_num <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-646"><a></a>        <span class="cf">return</span> {</span>
<span id="cb11-647"><a></a>            <span class="st">'metrics'</span>: metrics,</span>
<span id="cb11-648"><a></a>            <span class="st">'timers'</span>: timers,</span>
<span id="cb11-649"><a></a>            <span class="st">'x'</span>: x,</span>
<span id="cb11-650"><a></a>            <span class="st">'y'</span>: y,</span>
<span id="cb11-651"><a></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-3-5">
<div class="sourceCode" id="cb12" data-startfrom="500"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python" style="counter-reset: source-line 499;"><span id="cb12-500"><a></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb12-501"><a></a>    <span class="kw">def</span> estimate_loss(<span class="va">self</span>):</span>
<span id="cb12-502"><a></a>        out <span class="op">=</span> {}</span>
<span id="cb12-503"><a></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb12-504"><a></a>        <span class="cf">for</span> split <span class="kw">in</span> <span class="va">self</span>.config.data.data.keys():</span>
<span id="cb12-505"><a></a>            losses <span class="op">=</span> torch.zeros(<span class="va">self</span>.config.train.eval_iters)</span>
<span id="cb12-506"><a></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.config.train.eval_iters):</span>
<span id="cb12-507"><a></a>                x, y <span class="op">=</span> <span class="va">self</span>.get_batch(split)</span>
<span id="cb12-508"><a></a>                <span class="cf">with</span> <span class="va">self</span>.config.ctx:</span>
<span id="cb12-509"><a></a>                    _, loss <span class="op">=</span> <span class="va">self</span>.model_engine(x, y)</span>
<span id="cb12-510"><a></a>                losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb12-511"><a></a>            out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb12-512"><a></a>        <span class="va">self</span>.model.train()</span>
<span id="cb12-513"><a></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="self-contained-shakespeare-example" class="slide level2 center">
<h2>Self-Contained Shakespeare Example</h2>
<div class="quarto-layout-panel" data-layout="[ 60, -5, 25 ]">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 66.7%;justify-content: flex-start;">
<div class="panel-tabset" style="font-size: 0.9em; width: 100%!important; height: 100%!important;">
<ul id="tabset-4" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-4-1">üìí Shakespeare</a></li><li><a href="#tabset-4-2">üîó Links</a></li></ul>
<div class="tab-content" style="font-size: 0.9em; width: 100%!important; height: 100%!important;">
<div id="tabset-4-1">
<ul>
<li><a href="https://colab.research.google.com/github/saforem2/wordplay/blob/master/notebooks/shakespeare.ipynb"><img data-src="https://colab.research.google.com/assets/colab-badge.svg"></a><br>
</li>
<li><a href="https://github.com/saforem2/wordplay/blob/main/notebooks/shakespeare.ipynb"><code>shakespeare.ipynb</code></a></li>
</ul>
</div>
<div id="tabset-4-2">
<ul>
<li><span style="background-color:#f8f8f8; padding: 2pt; border-radius: 6pt">üìä <a href="https://saforem2.github.io/llm-workshop-talk/#/llm-workshop-talk">Slides</a></span></li>
<li><span style="background-color:#f8f8f8; padding: 2pt; border-radius: 6pt">üè° <a href="https://saforem2.github.io/wordplay">Project Website</a></span></li>
<li><span style="background-color:#f8f8f8; padding: 2pt; border-radius: 6pt">üíª <a href="https://github.com/saforem2/wordplay"><code>saforem2/wordplay</code></a></span></li>
</ul>
</div>
</div>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.6%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 27.8%;justify-content: flex-start;">
<!-- ![(link to Colab Notebook)](./assets/qrcode-colab.png) -->
</div>
</div>
</div>
</section>
<section id="section" class="slide level2 center" data-background-iframe="https://saforem2.github.io/wordplay">
<h2></h2>
</section>
<section id="links-1" class="slide level2 center">
<h2>Links</h2>
<ol type="1">
<li><a href="https://github.com/Hannibal046/Awesome-LLM/blob/main/README.md"><i class="fa-brands fa-github" aria-label="github"></i> Hannibal046/Awesome-LLM</a> <span class="inline-image"><a href="https://awesome.re"><img data-src="https://awesome.re/badge.svg" alt="Awesome"></a></span></li>
<li><a href="https://github.com/Mooler0410/LLMsPracticalGuide"><i class="fa-brands fa-github" aria-label="github"></i> Mooler0410/LLMsPracticalGuide</a></li>
<li><a href="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g238b2698243_0_734https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g238b2698243_0_734">Large Language Models (in 2023)</a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://ig.ft.com/generative-ai/">Generative AI Exists because of the Transformer</a></li>
<li><a href="https://jaykmody.com/blog/gpt-from-scratch/">GPT in 60 Lines of Numpy</a></li>
<li><a href="https://openai.com/research/better-language-models">Better Language Models and their Implications</a><br>
</li>
<li><span class="green-text"><i class="fa-solid fa-flask-vial" aria-label="flask-vial"></i></span> <a href="https://bigscience.notion.site/ebe3760ae1724dcc92f2e6877de0938f?v=2faf85dc00794321be14bc892539dd4f">Progress / Artefacts / Outcomes from üå∏ Bloom BigScience</a></li>
</ol>
<div title="Acknowledgements">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Acknowledgements</strong></p>
</div>
<div class="callout-content">
<p>This research used resources of the Argonne Leadership Computing Facility,<br>
which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.</p>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<ol type="1">
<li><p><a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">DeepSpeed: Extreme-scale model training for everyone - Microsoft Research</a></p></li>
<li><p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html">NVIDIA / NCCL / Collective Operations</a></p></li>
</ol>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-yang2023harnessing" class="csl-entry" role="listitem">
Yang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. <span>‚ÄúHarnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.‚Äù</span> <a href="https://arxiv.org/abs/2304.13712">https://arxiv.org/abs/2304.13712</a>.
</div>
<div id="ref-yao2023tree" class="csl-entry" role="listitem">
Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. <span>‚ÄúTree of Thoughts: Deliberate Problem Solving with Large Language Models.‚Äù</span> <a href="https://arxiv.org/abs/2305.10601">https://arxiv.org/abs/2305.10601</a>.
</div>
</div>

<div class="quarto-auto-generated-content">
<p><img src="https://raw.githubusercontent.com/saforem2/llm-lunch-talk/main/docs/assets/anl.svg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
</section></section>

    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: false,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: true,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>